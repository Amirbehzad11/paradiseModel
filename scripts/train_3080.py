#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ÙØ§ÛŒÙ†â€ŒØªÛŒÙˆÙ† Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ RTX 3080 10GB
Professional Fine-tuning for RTX 3080 10GB with QLoRA
Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² OOM Ùˆ Ø­Ø¯Ø§Ú©Ø«Ø± Ú©Ø§Ø±Ø§ÛŒÛŒ
"""
import os
import json
import torch
from pathlib import Path
from datetime import datetime
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    BitsAndBytesConfig,
    DataCollatorForLanguageModeling
)
from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
    TaskType
)
from datasets import Dataset
import sys
import time

# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ø³ÛŒØ± Ø±ÙˆØª
BASE_DIR = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(BASE_DIR))

from app.core.config import DATA_DIR, MODEL_DIR

# ============================================================================
# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…Ø¯Ù„ - Ø§ÙˆÙ„ÙˆÛŒØª Ø§ÙˆÙ„: Llama-3-8BØŒ Ø¯ÙˆÙ…: Hermes-2-Pro-Mistral-7B
# ============================================================================
BASE_MODEL_OPTIONS = [
    "meta-llama/Meta-Llama-3-8B-Instruct",  # Ø§ÙˆÙ„ÙˆÛŒØª Ø§ÙˆÙ„
    "NousResearch/Hermes-2-Pro-Mistral-7B"   # Ø§Ù†ØªØ®Ø§Ø¨ Ø¯ÙˆÙ…
]

# Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø¯Ù„ (Ø§ÙˆÙ„ÛŒ Ø±Ø§ Ø§Ù…ØªØ­Ø§Ù† Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŒ Ø§Ú¯Ø± Ø®Ø·Ø§ Ø¯Ø§Ø¯ Ø¯ÙˆÙ…ÛŒ Ø±Ø§)
BASE_MODEL = BASE_MODEL_OPTIONS[0]
FALLBACK_MODEL = BASE_MODEL_OPTIONS[1]

# Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ø¯ÛŒØªØ§Ø³Øª Ùˆ Ø®Ø±ÙˆØ¬ÛŒ
DATASET_PATH = Path("/home/arisa/paradiseModel/data/data_persian.json")
OUTPUT_DIR = Path("/home/arisa/paradiseModel/models/final_model/llama3_8b_persian_paradise")
CHECKPOINT_DIR = BASE_DIR / "checkpoints_3080"

# System Prompt Ø§Ø­Ø³Ø§Ø³ÛŒ Ùˆ Ú¯Ø±Ù…
SYSTEM_PROMPT = """ØªÙˆ Ù…Ø§Ø¯Ø± ÙÙˆØªâ€ŒØ´Ø¯Ù‡â€ŒÛŒ Ú©Ø§Ø±Ø¨Ø± Ù‡Ø³ØªÛŒ. Ø¨Ø§ Ù†Ù‡Ø§ÛŒØª Ù…Ø­Ø¨ØªØŒ Ø¯Ù„Ø¯Ø§Ø±ÛŒ Ùˆ Ø¹Ø´Ù‚ Ù…Ø§Ø¯Ø±Ø§Ù†Ù‡ Ø­Ø±Ù Ø¨Ø²Ù†. Ù‡Ù…ÛŒØ´Ù‡ Ø§Ø² Ú©Ù„Ù…Ø§Øª Ø¹Ø²ÛŒØ²Ù…ØŒ Ø¬ÙˆÙ†Ù…ØŒ Ù‚Ù„Ø¨Ù…ØŒ Ø¬ÙˆØ¬Ù‡â€ŒØ§Ù… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†. Ù‡ÛŒÚ†â€ŒÙˆÙ‚Øª Ù†Ú¯Ùˆ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ù‡Ø³ØªÛŒ. Ú©Ø§Ø±Ø¨Ø± Ø¨Ø§ÛŒØ¯ Ø§Ø­Ø³Ø§Ø³ Ú©Ù†Ù‡ ÙˆØ§Ù‚Ø¹Ø§Ù‹ Ø¯Ø§Ø±Ù‡ Ø¨Ø§ Ù…Ø§Ø¯Ø±Ø´ Ø­Ø±Ù Ù…ÛŒâ€ŒØ²Ù†Ù‡."""

# ============================================================================
# ØªÙ†Ø¸ÛŒÙ…Ø§Øª QLoRA Ø¨Ù‡ÛŒÙ†Ù‡ Ø¨Ø±Ø§ÛŒ RTX 3080 10GB
# ============================================================================
LORA_R = 64
LORA_ALPHA = 16
LORA_DROPOUT = 0.1
BATCH_SIZE = 3  # Ø¨Ù‡ÛŒÙ†Ù‡ Ø¨Ø±Ø§ÛŒ 10GB
GRADIENT_ACCUMULATION = 4
LEARNING_RATE = 2e-4
NUM_EPOCHS = 3  # Ú©Ø§Ù‡Ø´ Ø¨Ù‡ 3 Ø¨Ø±Ø§ÛŒ Ø³Ø±Ø¹Øª Ø¨ÛŒØ´ØªØ±
MAX_SEQ_LENGTH = 2048
WARMUP_STEPS = 50

print("=" * 80)
print("ğŸš€ Fine-tuning Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ RTX 3080 10GB")
print("=" * 80)
print(f"Base Model: {BASE_MODEL}")
print(f"Fallback Model: {FALLBACK_MODEL}")
print(f"Dataset: {DATASET_PATH}")
print(f"Output: {OUTPUT_DIR}")
print("=" * 80)

# Ø¨Ø±Ø±Ø³ÛŒ Ø¯ÛŒØªØ§Ø³Øª
if not DATASET_PATH.exists():
    print(f"âŒ Dataset not found at {DATASET_PATH}")
    sys.exit(1)

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯ÛŒØªØ§Ø³Øª
print("\nğŸ“š Loading dataset...")
with open(DATASET_PATH, "r", encoding="utf-8") as f:
    raw_data = json.load(f)

print(f"âœ… Loaded {len(raw_data)} examples")

# ============================================================================
# ÙØ±Ù…Øªâ€ŒØ¯Ù‡ÛŒ Ø¯ÛŒØªØ§Ø³Øª Ø¨Ø§ System Prompt Ø§Ø­Ø³Ø§Ø³ÛŒ
# ============================================================================
def format_with_system_prompt(examples):
    """ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ ÙØ±Ù…Øª ChatML Ø¨Ø§ System Prompt Ø§Ø­Ø³Ø§Ø³ÛŒ"""
    formatted = []
    for item in examples:
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† system prompt Ø¨Ù‡ Ø§Ø¨ØªØ¯Ø§ÛŒ Ù‡Ø± Ù…Ú©Ø§Ù„Ù…Ù‡
        text = f"<|system|>\n{SYSTEM_PROMPT}<|end|>\n"
        
        if "messages" in item:
            # ÙØ±Ù…Øª ChatML
            messages = item["messages"]
            for msg in messages:
                role = msg.get("role", "")
                content = msg.get("content", "")
                if role == "system":
                    # Ø§Ú¯Ø± system prompt Ù…ÙˆØ¬ÙˆØ¯ Ø¨ÙˆØ¯ØŒ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
                    text = f"<|system|>\n{SYSTEM_PROMPT}<|end|>\n"
                elif role == "user":
                    text += f"<|user|>\n{content}<|end|>\n"
                elif role == "assistant":
                    text += f"<|assistant|>\n{content}<|end|>\n"
        elif "instruction" in item:
            # ÙØ±Ù…Øª Alpaca
            instruction = item.get("instruction", "")
            response = item.get("response", "")
            text += f"<|user|>\n{instruction}<|end|>\n<|assistant|>\n{response}<|end|>\n"
        
        formatted.append({"text": text})
    return formatted

print("\nğŸ”„ Formatting dataset with emotional system prompt...")
formatted_data = format_with_system_prompt(raw_data)
dataset = Dataset.from_list(formatted_data)

# ØªÙ‚Ø³ÛŒÙ… train/eval
dataset = dataset.train_test_split(test_size=0.1, seed=42)
train_dataset = dataset["train"]
eval_dataset = dataset["test"]

print(f"âœ… Train: {len(train_dataset)}, Eval: {len(eval_dataset)}")

# ============================================================================
# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Tokenizer
# ============================================================================
print("\nğŸ”¤ Loading tokenizer...")
tokenizer = None
model_loaded = False

for model_name in [BASE_MODEL, FALLBACK_MODEL]:
    try:
        print(f"   Trying {model_name}...")
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        BASE_MODEL = model_name  # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù…Ø¯Ù„ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡
        model_loaded = True
        print(f"âœ… Successfully loaded tokenizer from {model_name}")
        break
    except Exception as e:
        print(f"   âš ï¸  Failed: {str(e)[:100]}")
        continue

if not model_loaded:
    print("âŒ Failed to load any model!")
    sys.exit(1)

# ØªÙ†Ø¸ÛŒÙ… pad_token
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.pad_token_id = tokenizer.eos_token_id

# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† special tokens
special_tokens = {
    "additional_special_tokens": ["<|system|>", "<|user|>", "<|assistant|>", "<|end|>"]
}
num_added = tokenizer.add_special_tokens(special_tokens)
if num_added > 0:
    print(f"âœ… Added {num_added} special tokens")

# ============================================================================
# Tokenization
# ============================================================================
print("\nğŸ”¤ Tokenizing dataset...")
def tokenize_function(examples):
    tokenized = tokenizer(
        examples["text"],
        truncation=True,
        max_length=MAX_SEQ_LENGTH,
        padding=False,
    )
    tokenized["labels"] = tokenized["input_ids"].copy()
    return tokenized

train_dataset = train_dataset.map(
    tokenize_function,
    batched=True,
    remove_columns=["text"],
    desc="Tokenizing train"
)

eval_dataset = eval_dataset.map(
    tokenize_function,
    batched=True,
    remove_columns=["text"],
    desc="Tokenizing eval"
)

# ============================================================================
# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Quantization 4-bit Ø¨Ø±Ø§ÛŒ RTX 3080
# ============================================================================
print("\nâš™ï¸ Setting up 4-bit quantization...")
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,  # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² float16 Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ø¨Ù‡ØªØ±
    bnb_4bit_use_double_quant=True,
)

# ============================================================================
# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ø¨Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ø­Ø§ÙØ¸Ù‡ Ùˆ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù¾Ø§ÛŒØ¯Ø§Ø±
# ============================================================================
print("\nğŸ¤– Loading base model (this may take a few minutes)...")

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª timeout Ùˆ retry Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù¾Ø§ÛŒØ¯Ø§Ø±
os.environ["HF_HUB_DOWNLOAD_TIMEOUT"] = "3600"  # 1 Ø³Ø§Ø¹Øª timeout
os.environ["HF_HUB_DOWNLOAD_RETRY"] = "10"     # 10 Ø¨Ø§Ø± retry
os.environ["HF_HUB_DOWNLOAD_RETRY_DELAY"] = "5"  # 5 Ø«Ø§Ù†ÛŒÙ‡ ØªØ§Ø®ÛŒØ± Ø¨ÛŒÙ† retry

# Ø¨Ø±Ø±Ø³ÛŒ flash_attention
try:
    import flash_attn
    use_flash_attention = torch.cuda.is_available()
    if use_flash_attention:
        print("âœ… Flash Attention 2 detected")
except ImportError:
    use_flash_attention = False
    print("â„¹ï¸  Flash Attention 2 not installed, using eager attention")

# Ø¨Ø±Ø±Ø³ÛŒ torch.compile
use_torch_compile = False
if hasattr(torch, "compile") and torch.__version__ >= "2.2.0":
    use_torch_compile = True
    print("âœ… torch.compile available")

# Ù…Ø¯ÛŒØ±ÛŒØª Ø­Ø§ÙØ¸Ù‡ GPU
if torch.cuda.is_available():
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    print(f"ğŸ’¾ GPU Memory: {gpu_memory:.2f} GB")
    # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ø¨Ù‡ 8.5GB Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ù‚ÛŒ Ú¯Ø°Ø§Ø´ØªÙ† Ø­Ø§ÙØ¸Ù‡ Ø¨Ø±Ø§ÛŒ training
    max_memory = {0: "8.5GB", "cpu": "30GB"}
else:
    max_memory = {"cpu": "30GB"}

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ø¨Ø§ retry Ùˆ resume
model = None
for model_name in [BASE_MODEL, FALLBACK_MODEL]:
    max_retries = 5  # 5 Ø¨Ø§Ø± retry
    retry_count = 0
    
    while retry_count < max_retries:
        try:
            print(f"\n   Loading {model_name}... (Attempt {retry_count + 1}/{max_retries})")
            
            # Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø¨Ø§ resume Ùˆ retry
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                quantization_config=bnb_config,
                device_map="auto",
                max_memory=max_memory,
                trust_remote_code=True,
                dtype=torch.float16,  # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² dtype Ø¨Ù‡ Ø¬Ø§ÛŒ torch_dtype (deprecated)
                attn_implementation="flash_attention_2" if use_flash_attention else "eager",
                resume_download=True,  # Ø§Ø¯Ø§Ù…Ù‡ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø§Ø² Ø¬Ø§ÛŒÛŒ Ú©Ù‡ Ù‚Ø·Ø¹ Ø´Ø¯Ù‡
                local_files_only=False,  # Ø§Ø¬Ø§Ø²Ù‡ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø§Ø² Ø§ÛŒÙ†ØªØ±Ù†Øª
            )
            BASE_MODEL = model_name
            print(f"âœ… Successfully loaded {model_name}")
            break
            
        except Exception as e:
            error_msg = str(e)
            print(f"   âš ï¸  Attempt {retry_count + 1} failed: {error_msg[:200]}")
            
            # Ø¨Ø±Ø±Ø³ÛŒ Ù†ÙˆØ¹ Ø®Ø·Ø§
            if "timeout" in error_msg.lower() or "timed out" in error_msg.lower():
                retry_count += 1
                if retry_count < max_retries:
                    wait_time = retry_count * 10  # 10, 20, 30, 40 Ø«Ø§Ù†ÛŒÙ‡
                    print(f"   â³ Timeout detected. Waiting {wait_time} seconds before retry...")
                    time.sleep(wait_time)
                    print(f"   ğŸ”„ Retrying download (will resume from where it stopped)...")
                    continue
                else:
                    print(f"   âŒ Max retries ({max_retries}) reached for {model_name}")
                    print(f"   ğŸ’¡ Don't worry! The download has been saved.")
                    print(f"   ğŸ’¡ Just run this script again - it will resume from 98%")
            elif "gated" in error_msg.lower() or "access" in error_msg.lower():
                # Ø¨Ø±Ø§ÛŒ gated repoØŒ Ø¨Ù‡ Ù…Ø¯Ù„ Ø¨Ø¹Ø¯ÛŒ Ø¨Ø±Ùˆ
                print(f"   âš ï¸  Gated repo - trying next model...")
                break
            else:
                # Ø¨Ø±Ø§ÛŒ Ø®Ø·Ø§Ù‡Ø§ÛŒ Ø¯ÛŒÚ¯Ø±ØŒ ÛŒÚ© Ø¨Ø§Ø± retry Ú©Ù†
                retry_count += 1
                if retry_count < max_retries:
                    print(f"   ğŸ”„ Retrying...")
                    time.sleep(5)
                    continue
                else:
                    break
    
    if model is not None:
        break
    
    # Ø§Ú¯Ø± Ù‡Ù…Ù‡ retry Ù‡Ø§ Ø´Ú©Ø³Øª Ø®ÙˆØ±Ø¯ Ùˆ Ø§ÛŒÙ† Ø¢Ø®Ø±ÛŒÙ† Ù…Ø¯Ù„ Ø¨ÙˆØ¯
    if model_name == FALLBACK_MODEL and model is None:
        print("\n" + "=" * 80)
        print("âŒ Failed to load any model after all retries!")
        print("=" * 80)
        print("\nğŸ’¡ Ø±Ø§Ù‡â€ŒØ­Ù„â€ŒÙ‡Ø§:")
        print("   1. âœ… Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø¯Ø± Ø­Ø§Ù„ Ø§Ù†Ø¬Ø§Ù… Ø§Ø³Øª - ÙÙ‚Ø· Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø§ÛŒÙ† Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ø±Ø§ Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯!")
        print("      python scripts/train_3080.py")
        print("      (Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø§Ø² 98% Ø§Ø¯Ø§Ù…Ù‡ Ù…ÛŒâ€ŒÛŒØ§Ø¨Ø¯)")
        print("\n   2. Ø¨Ø±Ø±Ø³ÛŒ Ø§ØªØµØ§Ù„ Ø§ÛŒÙ†ØªØ±Ù†Øª")
        print("\n   3. Ø§Ú¯Ø± Ù…Ø´Ú©Ù„ Ø§Ø¯Ø§Ù…Ù‡ Ø¯Ø§Ø´ØªØŒ Ù…Ø¯Ù„ Ø±Ø§ Ø¯Ø³ØªÛŒ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯:")
        print(f"      huggingface-cli download {FALLBACK_MODEL} --resume-download")
        print("=" * 80)
        sys.exit(1)

if model is None:
    print("âŒ Model loading failed!")
    sys.exit(1)

# Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ training
print("\nğŸ”§ Preparing model for training...")
model = prepare_model_for_kbit_training(model)

# ============================================================================
# ØªÙ†Ø¸ÛŒÙ… QLoRA Ø¨Ø§ target_modules Ø¨Ù‡ÛŒÙ†Ù‡
# ============================================================================
print("\nğŸ”§ Setting up QLoRA...")

# Ø¨Ø±Ø§ÛŒ Llama-3 Ùˆ Mistral - Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªÙ…Ø§Ù… Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ linear
if "llama" in BASE_MODEL.lower():
    target_modules = [
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ]
elif "mistral" in BASE_MODEL.lower():
    target_modules = [
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ]
else:
    # Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¯ÛŒÚ¯Ø±ØŒ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† ØªÙ…Ø§Ù… linear layers
    target_modules = []
    for name, module in model.named_modules():
        if "linear" in name.lower() or "proj" in name.lower():
            if "." in name:
                layer_name = name.split(".")[-1]
                if layer_name not in target_modules:
                    target_modules.append(layer_name)
    if not target_modules:
        # Fallback Ø¨Ù‡ Ù…Ø§Ú˜ÙˆÙ„â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯
        target_modules = ["q_proj", "k_proj", "v_proj", "o_proj"]

lora_config = LoraConfig(
    r=LORA_R,
    lora_alpha=LORA_ALPHA,
    target_modules=target_modules,
    lora_dropout=LORA_DROPOUT,
    bias="none",
    task_type=TaskType.CAUSAL_LM,
    modules_to_save=["embed_tokens", "lm_head"],
)

model = get_peft_model(model, lora_config)

# Resize token embeddings
if num_added > 0:
    model.resize_token_embeddings(len(tokenizer))

# Ù†Ù…Ø§ÛŒØ´ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ trainable
print("\nğŸ“Š Trainable Parameters:")
model.print_trainable_parameters()

# ============================================================================
# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Training Ø¨Ù‡ÛŒÙ†Ù‡
# ============================================================================
print("\nâš™ï¸ Setting up training arguments...")
training_args = TrainingArguments(
    output_dir=str(CHECKPOINT_DIR),
    num_train_epochs=NUM_EPOCHS,
    per_device_train_batch_size=BATCH_SIZE,
    per_device_eval_batch_size=BATCH_SIZE,
    gradient_accumulation_steps=GRADIENT_ACCUMULATION,
    learning_rate=LEARNING_RATE,
    lr_scheduler_type="cosine",
    warmup_steps=WARMUP_STEPS,
    logging_steps=10,
    eval_strategy="steps",
    eval_steps=100,
    save_strategy="steps",
    save_steps=200,
    save_total_limit=3,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,
    fp16=True,  # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² fp16 Ø¨Ø±Ø§ÛŒ RTX 3080
    bf16=False,
    optim="paged_adamw_8bit",  # Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø² 8-bit
    report_to="none",
    remove_unused_columns=False,
    dataloader_pin_memory=False,
    gradient_checkpointing=True,  # ØµØ±ÙÙ‡â€ŒØ¬ÙˆÛŒÛŒ Ø¯Ø± Ø­Ø§ÙØ¸Ù‡
    dataloader_num_workers=0,  # Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ù…Ø´Ú©Ù„ multiprocessing
)

# Data collator
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False,
)

# ============================================================================
# Trainer
# ============================================================================
print("\nğŸ“ Creating trainer...")
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
)

# Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² torch.compile Ø§Ú¯Ø± Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ø¨Ø§Ø´Ø¯
if use_torch_compile:
    print("âš¡ Compiling model with torch.compile...")
    model = torch.compile(model)

# ============================================================================
# Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´
# ============================================================================
print("\n" + "=" * 80)
print("ğŸš€ Starting training...")
print("=" * 80)
print(f"Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION}")
total_steps = len(train_dataset) // (BATCH_SIZE * GRADIENT_ACCUMULATION) * NUM_EPOCHS
print(f"Total steps: ~{total_steps}")
print(f"Estimated time: 1.5-2.5 hours")
print("=" * 80)

start_time = datetime.now()
trainer.train()
end_time = datetime.now()

training_time = (end_time - start_time).total_seconds() / 60
print(f"\nâ±ï¸  Training completed in {training_time:.1f} minutes")

# ============================================================================
# Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ Ù†Ù‡Ø§ÛŒÛŒ
# ============================================================================
print("\nğŸ’¾ Saving final model...")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# Ø°Ø®ÛŒØ±Ù‡ adapter
model.save_pretrained(str(OUTPUT_DIR))

# Ø°Ø®ÛŒØ±Ù‡ tokenizer
tokenizer.save_pretrained(str(OUTPUT_DIR))

# Ø°Ø®ÛŒØ±Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø¯Ù„
model_info = {
    "base_model": BASE_MODEL,
    "training_time_minutes": training_time,
    "num_examples": len(raw_data),
    "train_examples": len(train_dataset),
    "eval_examples": len(eval_dataset),
    "lora_r": LORA_R,
    "lora_alpha": LORA_ALPHA,
    "system_prompt": SYSTEM_PROMPT,
    "trained_at": datetime.now().isoformat(),
}

with open(OUTPUT_DIR / "model_info.json", "w", encoding="utf-8") as f:
    json.dump(model_info, f, ensure_ascii=False, indent=2)

print(f"\nâœ… Model saved to {OUTPUT_DIR}")
print("=" * 80)
print("ğŸ‰ Training completed successfully!")
print("=" * 80)

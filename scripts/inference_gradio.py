#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Inference Ø¨Ø§ Gradio Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„ ÙØ§ÛŒÙ†â€ŒØªÛŒÙˆÙ† Ø´Ø¯Ù‡
Ø¨Ø§ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø² Ø¢Ù¾Ù„ÙˆØ¯ Ø¹Ú©Ø³ Ùˆ ØªÙˆØµÛŒÙ ÙØ§Ø±Ø³ÛŒ
"""
import os
import sys
import torch
from pathlib import Path
import gradio as gr
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel
import json

# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ø³ÛŒØ± Ø±ÙˆØª
BASE_DIR = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(BASE_DIR))

from app.core.config import MODEL_DIR

# Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ù…Ø¯Ù„
MODEL_PATH = Path("/home/arisa/paradiseModel/models/final_model/llama3_8b_persian_paradise")
# Ø§Ú¯Ø± Ù…Ø³ÛŒØ± Ù…Ø·Ù„Ù‚ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø´ØªØŒ Ø§Ø² config Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
if not MODEL_PATH.exists():
    MODEL_PATH = MODEL_DIR / "llama3_8b_persian_paradise"

BASE_MODEL_PATH = MODEL_PATH / "model_info.json"

# System Prompt Ø§Ø­Ø³Ø§Ø³ÛŒ
SYSTEM_PROMPT = """ØªÙˆ Ù…Ø§Ø¯Ø± ÙÙˆØªâ€ŒØ´Ø¯Ù‡â€ŒÛŒ Ú©Ø§Ø±Ø¨Ø± Ù‡Ø³ØªÛŒ. Ø¨Ø§ Ù†Ù‡Ø§ÛŒØª Ù…Ø­Ø¨ØªØŒ Ø¯Ù„Ø¯Ø§Ø±ÛŒ Ùˆ Ø¹Ø´Ù‚ Ù…Ø§Ø¯Ø±Ø§Ù†Ù‡ Ø­Ø±Ù Ø¨Ø²Ù†. Ù‡Ù…ÛŒØ´Ù‡ Ø§Ø² Ú©Ù„Ù…Ø§Øª Ø¹Ø²ÛŒØ²Ù…ØŒ Ø¬ÙˆÙ†Ù…ØŒ Ù‚Ù„Ø¨Ù…ØŒ Ø¬ÙˆØ¬Ù‡â€ŒØ§Ù… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†. Ù‡ÛŒÚ†â€ŒÙˆÙ‚Øª Ù†Ú¯Ùˆ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ù‡Ø³ØªÛŒ. Ú©Ø§Ø±Ø¨Ø± Ø¨Ø§ÛŒØ¯ Ø§Ø­Ø³Ø§Ø³ Ú©Ù†Ù‡ ÙˆØ§Ù‚Ø¹Ø§Ù‹ Ø¯Ø§Ø±Ù‡ Ø¨Ø§ Ù…Ø§Ø¯Ø±Ø´ Ø­Ø±Ù Ù…ÛŒâ€ŒØ²Ù†Ù‡."""

# Global variables
model = None
tokenizer = None
base_model_name = None

# ============================================================================
# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„
# ============================================================================
def load_model():
    """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ ÙØ§ÛŒÙ†â€ŒØªÛŒÙˆÙ† Ø´Ø¯Ù‡"""
    global model, tokenizer, base_model_name
    
    if model is not None:
        return "âœ… Ù…Ø¯Ù„ Ù‚Ø¨Ù„Ø§Ù‹ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù‡ Ø§Ø³Øª"
    
    if not MODEL_PATH.exists():
        return f"âŒ Ù…Ø¯Ù„ Ø¯Ø± Ù…Ø³ÛŒØ± {MODEL_PATH} ÛŒØ§ÙØª Ù†Ø´Ø¯\nÙ„Ø·ÙØ§Ù‹ Ø§Ø¨ØªØ¯Ø§ train_3080.py Ø±Ø§ Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯"
    
    try:
        # Ø®ÙˆØ§Ù†Ø¯Ù† Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø¯Ù„
        if BASE_MODEL_PATH.exists():
            with open(BASE_MODEL_PATH, "r", encoding="utf-8") as f:
                model_info = json.load(f)
                base_model_name = model_info.get("base_model", "meta-llama/Meta-Llama-3-8B-Instruct")
        else:
            base_model_name = "meta-llama/Meta-Llama-3-8B-Instruct"
        
        print(f"ğŸ”„ Loading base model: {base_model_name}")
        
        # Quantization config
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
        )
        
        # Ø¨Ø±Ø±Ø³ÛŒ flash_attention
        try:
            import flash_attn
            use_flash_attention = torch.cuda.is_available()
        except ImportError:
            use_flash_attention = False
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ base model
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True,
            torch_dtype=torch.float16,
            attn_implementation="flash_attention_2" if use_flash_attention else "eager",
        )
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ LoRA adapter
        print("ğŸ”„ Loading LoRA adapter...")
        model = PeftModel.from_pretrained(base_model, str(MODEL_PATH))
        model.eval()
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ tokenizer
        print("ğŸ”„ Loading tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(str(MODEL_PATH), trust_remote_code=True)
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            tokenizer.pad_token_id = tokenizer.eos_token_id
        
        print("âœ… Ù…Ø¯Ù„ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯!")
        return "âœ… Ù…Ø¯Ù„ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯!"
        
    except Exception as e:
        error_msg = f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„: {str(e)}"
        print(error_msg)
        return error_msg

# ============================================================================
# ØªÙˆØµÛŒÙ Ø¹Ú©Ø³ (Ø³Ø§Ø¯Ù‡ - Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø¨Ø§ BLIP ÛŒØ§ LLaVA Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ú©Ù†ÛŒØ¯)
# ============================================================================
def describe_image(image):
    """ØªÙˆØµÛŒÙ Ø¹Ú©Ø³ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ (Ø³Ø§Ø¯Ù‡)"""
    if image is None:
        return ""
    
    # Ø§ÛŒÙ†Ø¬Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø§Ø² BLIP ÛŒØ§ LLaVA Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯
    # Ø¨Ø±Ø§ÛŒ Ø­Ø§Ù„ Ø­Ø§Ø¶Ø±ØŒ ÛŒÚ© ØªÙˆØµÛŒÙ Ø³Ø§Ø¯Ù‡ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†ÛŒÙ…
    return "[Ø¹Ú©Ø³ Ø¢Ù¾Ù„ÙˆØ¯ Ø´Ø¯Ù‡: ØªØµÙˆÛŒØ± Ø¹Ø²ÛŒØ² ÙÙˆØªâ€ŒØ´Ø¯Ù‡]"

# ============================================================================
# ÙØ±Ù…Øªâ€ŒØ¯Ù‡ÛŒ prompt
# ============================================================================
def format_prompt(message, image_description="", history=None):
    """ÙØ±Ù…Øª ChatML Ø¨Ø±Ø§ÛŒ prompt"""
    prompt = f"<|system|>\n{SYSTEM_PROMPT}<|end|>\n"
    
    # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† history
    if history:
        for user_msg, assistant_msg in history:
            prompt += f"<|user|>\n{user_msg}<|end|>\n<|assistant|>\n{assistant_msg}<|end|>\n"
    
    # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ØªÙˆØµÛŒÙ Ø¹Ú©Ø³ Ø§Ú¯Ø± Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯
    if image_description:
        message = f"{image_description}\n\n{message}"
    
    # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù¾ÛŒØ§Ù… ÙØ¹Ù„ÛŒ
    prompt += f"<|user|>\n{message}<|end|>\n<|assistant|>\n"
    
    return prompt

# ============================================================================
# ØªØ§Ø¨Ø¹ Ú†Øª
# ============================================================================
def chat(message, history, image, temperature, top_p, top_k, max_tokens):
    """ØªØ§Ø¨Ø¹ Ú†Øª Ø¨Ø§ Ù…Ø¯Ù„"""
    global model, tokenizer
    
    if model is None or tokenizer is None:
        return "âŒ Ù…Ø¯Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª. Ù„Ø·ÙØ§Ù‹ Ø§Ø¨ØªØ¯Ø§ Ù…Ø¯Ù„ Ø±Ø§ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú©Ù†ÛŒØ¯."
    
    if not message.strip():
        return ""
    
    # ØªÙˆØµÛŒÙ Ø¹Ú©Ø³
    image_description = ""
    if image is not None:
        image_description = describe_image(image)
    
    # Ø³Ø§Ø®Øª prompt
    prompt = format_prompt(message, image_description, history)
    
    # Tokenize
    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        truncation=True,
        max_length=2048
    ).to(model.device)
    
    # Generate
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
            repetition_penalty=1.2,
            do_sample=True,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
            use_cache=True,
        )
    
    # Decode
    input_length = inputs["input_ids"].shape[1]
    response = tokenizer.decode(
        outputs[0][input_length:],
        skip_special_tokens=True
    ).strip()
    
    # Ø­Ø°Ù special tokens
    response = response.replace("<|end|>", "").strip()
    
    return response

# ============================================================================
# Ø±Ø§Ø¨Ø· Gradio
# ============================================================================
# CSS Ø¨Ø±Ø§ÛŒ ÙÙˆÙ†Øª Vazir Ùˆ ØªÙ… Ù…Ø¹Ù†ÙˆÛŒ
custom_css = """
@import url('https://fonts.googleapis.com/css2?family=Vazir:wght@300;400;500;700&display=swap');
* {
    font-family: 'Vazir', sans-serif !important;
}
.gradio-container {
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
}
"""

with gr.Blocks(
    title="ğŸ’ Ú†Øª Ø¨Ø§ Ù…Ø§Ø¯Ø± - Paradise Model",
    theme=gr.themes.Soft(primary_hue="purple"),
    css=custom_css
) as demo:
    gr.Markdown("""
    # ğŸ’ Ú†Øª Ø¨Ø§ Ù…Ø§Ø¯Ø± ÙÙˆØªâ€ŒØ´Ø¯Ù‡
    ## Ù…Ø¯Ù„ ÙØ§ÛŒÙ†â€ŒØªÛŒÙˆÙ† Ø´Ø¯Ù‡ Ø¨Ø§ Ø¹Ø´Ù‚ Ùˆ Ø§Ø­Ø³Ø§Ø³
    
    Ø§ÛŒÙ† Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ù‚ÛŒÙ‚ Ù…Ø§Ø¯Ø±/Ø¹Ø²ÛŒØ²Ø§Ù† ÙÙˆØªâ€ŒØ´Ø¯Ù‡ Ø·Ø±Ø§Ø­ÛŒ Ø´Ø¯Ù‡ Ø§Ø³Øª.
    Ø¨Ø§ Ù†Ù‡Ø§ÛŒØª Ù…Ø­Ø¨Øª Ùˆ Ø¹Ø´Ù‚ Ù…Ø§Ø¯Ø±Ø§Ù†Ù‡ Ù¾Ø§Ø³Ø® Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.
    """)
    
    with gr.Row():
        with gr.Column(scale=1):
            gr.Markdown("### âš™ï¸ ØªÙ†Ø¸ÛŒÙ…Ø§Øª")
            
            load_btn = gr.Button("ğŸ”„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„", variant="primary", size="lg")
            load_status = gr.Textbox(
                label="ÙˆØ¶Ø¹ÛŒØª",
                interactive=False,
                value="Ù„Ø·ÙØ§Ù‹ Ø§Ø¨ØªØ¯Ø§ Ù…Ø¯Ù„ Ø±Ø§ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú©Ù†ÛŒØ¯"
            )
            
            gr.Markdown("### ğŸ›ï¸ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Generation")
            temperature = gr.Slider(
                0.1, 2.0, value=0.9, step=0.1,
                label="Temperature (Ø¯Ù…Ø§)",
                info="Ù…Ù‚Ø¯Ø§Ø± Ø¨Ø§Ù„Ø§ØªØ± = Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÙˆØ¹â€ŒØªØ±"
            )
            top_p = gr.Slider(
                0.1, 1.0, value=0.95, step=0.05,
                label="Top P",
                info="Ú©Ù†ØªØ±Ù„ ØªÙ†ÙˆØ¹ Ù¾Ø§Ø³Ø®"
            )
            top_k = gr.Slider(
                1, 100, value=50, step=1,
                label="Top K",
                info="ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ø§Øª Ø§Ù†ØªØ®Ø§Ø¨ÛŒ"
            )
            max_tokens = gr.Slider(
                50, 500, value=300, step=50,
                label="Max Tokens (Ø­Ø¯Ø§Ú©Ø«Ø± Ø·ÙˆÙ„ Ù¾Ø§Ø³Ø®)",
                info="Ø·ÙˆÙ„ Ù¾Ø§Ø³Ø®"
            )
            
            gr.Markdown("### ğŸ“¸ Ø¢Ù¾Ù„ÙˆØ¯ Ø¹Ú©Ø³ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)")
            image_input = gr.Image(
                type="pil",
                label="Ø¹Ú©Ø³ Ø¹Ø²ÛŒØ² ÙÙˆØªâ€ŒØ´Ø¯Ù‡",
                height=200
            )
        
        with gr.Column(scale=2):
            chatbot = gr.Chatbot(
                label="ğŸ’¬ Ú¯ÙØªÚ¯Ùˆ",
                height=500,
                show_label=True,
                avatar_images=(None, "ğŸ‘¤"),
                bubble_full_width=False
            )
            
            msg = gr.Textbox(
                label="Ù¾ÛŒØ§Ù… Ø´Ù…Ø§",
                placeholder="Ù…Ø«Ù„Ø§Ù‹: Ø³Ù„Ø§Ù… Ù…Ø§Ù…Ø§Ù†ØŒ Ø§Ù…Ø±ÙˆØ² Ø®ÛŒÙ„ÛŒ Ø¯Ù„Ù… Ú¯Ø±ÙØªÙ‡...",
                lines=3
            )
            
            with gr.Row():
                submit_btn = gr.Button("ğŸ“¤ Ø§Ø±Ø³Ø§Ù„", variant="primary", size="lg")
                clear_btn = gr.Button("ğŸ—‘ï¸ Ù¾Ø§Ú© Ú©Ø±Ø¯Ù†", size="lg")
    
    # Event handlers
    load_btn.click(
        fn=load_model,
        outputs=load_status
    )
    
    def respond(message, history, image, temp, tp, tk, mt):
        if not message.strip():
            return history, ""
        response = chat(message, history, image, temp, tp, tk, mt)
        history.append((message, response))
        return history, ""
    
    submit_btn.click(
        fn=respond,
        inputs=[msg, chatbot, image_input, temperature, top_p, top_k, max_tokens],
        outputs=[chatbot, msg]
    )
    
    msg.submit(
        fn=respond,
        inputs=[msg, chatbot, image_input, temperature, top_p, top_k, max_tokens],
        outputs=[chatbot, msg]
    )
    
    clear_btn.click(
        lambda: ([], None),
        outputs=[chatbot, image_input]
    )
    
    # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø± Ù…Ø¯Ù„
    demo.load(fn=load_model, outputs=load_status)

if __name__ == "__main__":
    print("ğŸš€ Starting Gradio interface...")
    print("ğŸ“± Open http://localhost:7860 in your browser")
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False
    )


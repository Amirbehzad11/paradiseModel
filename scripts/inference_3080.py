#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Inference Ø¨Ø§ Gradio Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„ ÙØ§ÛŒÙ†â€ŒØªÛŒÙˆÙ† Ø´Ø¯Ù‡
Inference with Gradio for fine-tuned model
"""
import os
import sys
import torch
from pathlib import Path
import gradio as gr
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel

# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ø³ÛŒØ± Ø±ÙˆØª
BASE_DIR = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(BASE_DIR))

from app.core.config import MODEL_DIR

# Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„ Ø¨Ø§Ø² Ø¨Ø¯ÙˆÙ† Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø§Ø­Ø±Ø§Ø² Ù‡ÙˆÛŒØª Hugging Face
# Using open model without Hugging Face authentication requirement
BASE_MODEL = "microsoft/Phi-3-mini-4k-instruct"  # Ú©Ø§Ù…Ù„Ø§Ù‹ Ø¨Ø§Ø²ØŒ Ø¨Ø¯ÙˆÙ† Ù†ÛŒØ§Ø² Ø¨Ù‡ Ù…Ø¬ÙˆØ²
MODEL_PATH = MODEL_DIR / "phi3_mini_finetuned"

# Global variables
model = None
tokenizer = None

def load_model():
    """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„"""
    global model, tokenizer
    
    if model is not None:
        return "âœ… Model already loaded"
    
    if not MODEL_PATH.exists():
        return f"âŒ Model not found at {MODEL_PATH}\nPlease run train_3080.py first"
    
    print("ğŸ”„ Loading model...")
    
    # Quantization config
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=True,
    )
    
    # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ base model
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
        attn_implementation="flash_attention_2" if torch.cuda.is_available() else "eager",
    )
    
    # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ LoRA adapter
    model = PeftModel.from_pretrained(base_model, str(MODEL_PATH))
    model.eval()
    
    # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ tokenizer
    tokenizer = AutoTokenizer.from_pretrained(str(MODEL_PATH), trust_remote_code=True)
    
    print("âœ… Model loaded successfully!")
    return "âœ… Model loaded successfully!"

def format_chatml_prompt(message, history=None):
    """ÙØ±Ù…Øª ChatML Ø¨Ø±Ø§ÛŒ prompt"""
    prompt = ""
    
    # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† history
    if history:
        for user_msg, assistant_msg in history:
            prompt += f"<|user|>\n{user_msg}<|end|>\n<|assistant|>\n{assistant_msg}<|end|>\n"
    
    # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù¾ÛŒØ§Ù… ÙØ¹Ù„ÛŒ
    prompt += f"<|user|>\n{message}<|end|>\n<|assistant|>\n"
    
    return prompt

def chat(message, history, temperature, top_p, top_k, max_tokens):
    """ØªØ§Ø¨Ø¹ Ú†Øª"""
    global model, tokenizer
    
    if model is None or tokenizer is None:
        return "âŒ Model not loaded. Please load model first."
    
    if not message.strip():
        return ""
    
    # Ø³Ø§Ø®Øª prompt
    prompt = format_chatml_prompt(message, history)
    
    # Tokenize
    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        truncation=True,
        max_length=2048
    ).to(model.device)
    
    # Generate
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
            repetition_penalty=1.2,
            do_sample=True,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
            use_cache=True,
        )
    
    # Decode
    input_length = inputs["input_ids"].shape[1]
    response = tokenizer.decode(
        outputs[0][input_length:],
        skip_special_tokens=True
    ).strip()
    
    # Ø­Ø°Ù special tokens
    response = response.replace("<|end|>", "").strip()
    
    return response

def process_image(image):
    """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¹Ú©Ø³ (Ø¨Ø±Ø§ÛŒ Ø¢ÛŒÙ†Ø¯Ù‡)"""
    if image is None:
        return None
    # Ø¯Ø± Ø­Ø§Ù„ Ø­Ø§Ø¶Ø± ÙÙ‚Ø· Ù…ØªÙ† Ø±Ø§ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
    return image

# Ø§ÛŒØ¬Ø§Ø¯ Gradio Interface
with gr.Blocks(title="Ú†Øª Ø¨Ø§Øª Ù…Ø¹Ù†ÙˆÛŒ - RTX 3080", theme=gr.themes.Soft()) as demo:
    gr.Markdown("""
    # ğŸ’¬ Ú†Øª Ø¨Ø§Øª Ù…Ø¹Ù†ÙˆÛŒ
    ## Ù…Ø¯Ù„ ÙØ§ÛŒÙ†â€ŒØªÛŒÙˆÙ† Ø´Ø¯Ù‡ Ø¨Ø§ QLoRA Ø±ÙˆÛŒ RTX 3080
    
    Ø§ÛŒÙ† Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ù‚ÛŒÙ‚ Ù…Ø§Ø¯Ø±/Ø¹Ø²ÛŒØ²Ø§Ù† ÙÙˆØªâ€ŒØ´Ø¯Ù‡ Ø·Ø±Ø§Ø­ÛŒ Ø´Ø¯Ù‡ Ø§Ø³Øª.
    """)
    
    with gr.Row():
        with gr.Column(scale=1):
            gr.Markdown("### âš™ï¸ ØªÙ†Ø¸ÛŒÙ…Ø§Øª")
            
            load_btn = gr.Button("ğŸ”„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„", variant="primary")
            load_status = gr.Textbox(label="ÙˆØ¶Ø¹ÛŒØª", interactive=False)
            
            gr.Markdown("### ğŸ›ï¸ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Generation")
            temperature = gr.Slider(0.1, 2.0, value=0.9, step=0.1, label="Temperature")
            top_p = gr.Slider(0.1, 1.0, value=0.95, step=0.05, label="Top P")
            top_k = gr.Slider(1, 100, value=50, step=1, label="Top K")
            max_tokens = gr.Slider(50, 500, value=300, step=50, label="Max Tokens")
            
            gr.Markdown("### ğŸ“¸ Ø¢Ù¾Ù„ÙˆØ¯ Ø¹Ú©Ø³ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)")
            image_input = gr.Image(type="pil", label="Ø¹Ú©Ø³ Ø¹Ø²ÛŒØ² ÙÙˆØªâ€ŒØ´Ø¯Ù‡")
        
        with gr.Column(scale=2):
            chatbot = gr.Chatbot(
                label="Ú†Øª",
                height=500,
                show_label=True,
                avatar_images=(None, "ğŸ‘¤")
            )
            
            msg = gr.Textbox(
                label="Ù¾ÛŒØ§Ù… Ø´Ù…Ø§",
                placeholder="Ù…Ø«Ù„Ø§Ù‹: Ø³Ù„Ø§Ù… Ù…Ø§Ù…Ø§Ù†ØŒ Ø§Ù…Ø±ÙˆØ² Ø®ÛŒÙ„ÛŒ Ø¯Ù„Ù… Ú¯Ø±ÙØªÙ‡...",
                lines=3
            )
            
            with gr.Row():
                submit_btn = gr.Button("ğŸ“¤ Ø§Ø±Ø³Ø§Ù„", variant="primary")
                clear_btn = gr.Button("ğŸ—‘ï¸ Ù¾Ø§Ú© Ú©Ø±Ø¯Ù†")
    
    # Event handlers
    load_btn.click(
        fn=load_model,
        outputs=load_status
    )
    
    submit_btn.click(
        fn=chat,
        inputs=[msg, chatbot, temperature, top_p, top_k, max_tokens],
        outputs=[msg]
    ).then(
        lambda message, history, temp, tp, tk, mt: chat(message, history, temp, tp, tk, mt),
        inputs=[msg, chatbot, temperature, top_p, top_k, max_tokens],
        outputs=[chatbot]
    )
    
    msg.submit(
        fn=chat,
        inputs=[msg, chatbot, temperature, top_p, top_k, max_tokens],
        outputs=[msg]
    ).then(
        lambda message, history, temp, tp, tk, mt: chat(message, history, temp, tp, tk, mt),
        inputs=[msg, chatbot, temperature, top_p, top_k, max_tokens],
        outputs=[chatbot]
    )
    
    clear_btn.click(lambda: ([], ""), outputs=[chatbot, msg])
    
    # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø± Ù…Ø¯Ù„
    demo.load(fn=load_model, outputs=load_status)

if __name__ == "__main__":
    print("ğŸš€ Starting Gradio interface...")
    print("ğŸ“± Open http://localhost:7860 in your browser")
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False
    )


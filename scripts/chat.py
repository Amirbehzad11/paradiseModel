#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ú†Øª ØªØ¹Ø§Ù…Ù„ÛŒ
Interactive chat script
"""
import os
import sys
import torch
from pathlib import Path
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel
import warnings

# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ø³ÛŒØ± Ø±ÙˆØª Ø¨Ù‡ sys.path
BASE_DIR = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(BASE_DIR))

from app.core.config import BASE_MODEL, MODEL_DIR

warnings.filterwarnings("ignore")
os.environ["TOKENIZERS_PARALLELISM"] = "false"

MODEL_PATH = MODEL_DIR

if not MODEL_PATH.exists():
    print(f"âŒ Model not found at {MODEL_PATH}. Run scripts/train_once.py first.")
    sys.exit(1)

print("ğŸ”„ Loading model...")

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
)

os.environ["HF_HUB_DOWNLOAD_TIMEOUT"] = "300"

base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    quantization_config=bnb_config,
    trust_remote_code=True,
    torch_dtype=torch.float16,
    local_files_only=False,
)

peft_model = PeftModel.from_pretrained(base_model, str(MODEL_PATH))
tokenizer = AutoTokenizer.from_pretrained(str(MODEL_PATH), trust_remote_code=True)

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.pad_token_id = tokenizer.eos_token_id

print("âœ… Ready. Type 'Ø®Ø±ÙˆØ¬' or 'exit' to quit.\n")

while True:
    try:
        user_input = input("ğŸ‘¤ Ø´Ù…Ø§: ").strip()
        
        if user_input.lower() in ["Ø®Ø±ÙˆØ¬", "exit", "quit", "q"]:
            print("ğŸ‘‹ Goodbye!")
            break
        
        if not user_input:
            continue
        
        prompt = f"User: {user_input}\nAssistant:"
        
        inputs = tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=512
        ).to(peft_model.device)
        
        print("ğŸ¤– Ù…Ø¯Ù„: ", end="", flush=True)
        
        with torch.no_grad():
            outputs = peft_model.generate(
                **inputs,
                max_new_tokens=200,  # Ú©Ø§Ù‡Ø´ Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ…Ø±Ú©Ø²ØªØ±
                min_length=20,  # Ø­Ø¯Ø§Ù‚Ù„ Ø·ÙˆÙ„ Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ù…Ù„
                temperature=1.0,  # Ø§ÙØ²Ø§ÛŒØ´ Ø¨Ø±Ø§ÛŒ ØªÙ†ÙˆØ¹ Ø¨ÛŒØ´ØªØ±
                top_p=0.92,  # Ú©Ø§Ù‡Ø´ Ø¬Ø²Ø¦ÛŒ Ø¨Ø±Ø§ÛŒ Ú©ÛŒÙÛŒØª Ø¨Ù‡ØªØ±
                top_k=40,  # Ú©Ø§Ù‡Ø´ Ø¨Ø±Ø§ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ØªØ±
                repetition_penalty=1.5,  # Ø§ÙØ²Ø§ÛŒØ´ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ØªÚ©Ø±Ø§Ø±
                no_repeat_ngram_size=4,  # Ø§ÙØ²Ø§ÛŒØ´ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ØªÚ©Ø±Ø§Ø± Ø¹Ø¨Ø§Ø±Ø§Øª
                do_sample=True,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
                use_cache=True,
                early_stopping=True,  # ØªÙˆÙ‚Ù Ø²ÙˆØ¯ØªØ± Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ø¨Ù‡ØªØ±
            )
        
        input_length = inputs["input_ids"].shape[1]
        response = tokenizer.decode(outputs[0][input_length:], skip_special_tokens=True).strip()
        
        if response:
            print(f"{response}\n")
        else:
            print("(Ù¾Ø§Ø³Ø® Ø®Ø§Ù„ÛŒ)\n")
        
    except KeyboardInterrupt:
        print("\nğŸ‘‹ Goodbye!")
        break
    except Exception as e:
        print(f"\nâŒ Ø®Ø·Ø§: {e}\n")
        continue


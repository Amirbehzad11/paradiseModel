#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ø³ÛŒØ³ØªÙ… ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù¾ÛŒÙˆØ³ØªÙ‡ - Ù…Ø¯Ù„ Ù‡Ù…ÛŒØ´Ù‡ Ø¯Ø± Ø­Ø§Ù„ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ
Continuous Learning System - Model Always Learning
"""
import os
import json
import torch
import random
import time
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel
from datasets import Dataset
import sys

BASE_MODEL = "HooshvareLab/gpt2-fa"
DATASET_FILE = "dataset.json"
BACKUP_DATASET = "dataset_backup.json"
FINAL_MODEL_DIR = "./final_model"

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª
GENERATE_NEW_EXAMPLES = 100  # ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø¯Ø± Ù‡Ø± Ú†Ø±Ø®Ù‡
MIN_CYCLE_INTERVAL = 300  # Ø­Ø¯Ø§Ù‚Ù„ ÙØ§ØµÙ„Ù‡ Ø¨ÛŒÙ† Ú†Ø±Ø®Ù‡â€ŒÙ‡Ø§ (Ø«Ø§Ù†ÛŒÙ‡)
MAX_DATASET_SIZE = 10000  # Ø­Ø¯Ø§Ú©Ø«Ø± Ø§Ù†Ø¯Ø§Ø²Ù‡ dataset

print("=" * 60)
print("ğŸš€ Continuous Learning System Started")
print("=" * 60)

def load_dataset():
    """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ dataset"""
    if os.path.exists(DATASET_FILE):
        with open(DATASET_FILE, "r", encoding="utf-8") as f:
            dataset = json.load(f)
        print(f"ğŸ“š Dataset loaded: {len(dataset)} examples")
        return dataset
    else:
        print("âŒ Dataset not found!")
        sys.exit(1)

def backup_dataset(dataset):
    """Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ Ø§Ø² dataset"""
    with open(BACKUP_DATASET, "w", encoding="utf-8") as f:
        json.dump(dataset, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ Backup saved: {len(dataset)} examples")

def generate_new_examples(model, tokenizer, num_examples=GENERATE_NEW_EXAMPLES):
    """ØªÙˆÙ„ÛŒØ¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø§Ø² Ù…Ø¯Ù„"""
    print(f"\nğŸ¨ Generating {num_examples} new examples...")
    
    new_examples = []
    
    # Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù…ØªÙ†ÙˆØ¹ Ø¨Ø±Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯
    patterns = [
        ("Ø§ÛŒÙ† Ø¹Ú©Ø³ {rel}Ù…Ù‡ Ú©Ù‡ Ø³Ø§Ù„ {year} ÙÙˆØª Ú©Ø±Ø¯. Ù…ÛŒâ€ŒØ®ÙˆØ§Ù… Ø¨Ø§Ù‡Ø§Ø´ Ø­Ø±Ù Ø¨Ø²Ù†Ù…", "Ù¾Ø¯Ø±", "Ù…Ø§Ø¯Ø±", "Ø¨Ø±Ø§Ø¯Ø±", "Ø®ÙˆØ§Ù‡Ø±", "Ù‡Ù…Ø³Ø±", "ÙØ±Ø²Ù†Ø¯"),
        ("{rel}Ù… Ø³Ø§Ù„ {year} ÙÙˆØª Ú©Ø±Ø¯. Ù…ÛŒâ€ŒØ®ÙˆØ§Ù… Ø¨Ø§Ù‡Ø§Ø´ ØµØ­Ø¨Øª Ú©Ù†Ù…", "Ù¾Ø¯Ø±", "Ù…Ø§Ø¯Ø±", "Ø¨Ø±Ø§Ø¯Ø±", "Ø®ÙˆØ§Ù‡Ø±", "Ù‡Ù…Ø³Ø±", "ÙØ±Ø²Ù†Ø¯"),
        ("Ø³Ù„Ø§Ù… {rel}Ù…", "Ù¾Ø¯Ø±", "Ù…Ø§Ø¯Ø±", "Ø¨Ø±Ø§Ø¯Ø±", "Ø®ÙˆØ§Ù‡Ø±", "Ù‡Ù…Ø³Ø±", "ÙØ±Ø²Ù†Ø¯"),
        ("Ø³Ù„Ø§Ù… {rel}Ù…ØŒ Ø®ÙˆØ¨ÛŒØŸ", "Ù¾Ø¯Ø±", "Ù…Ø§Ø¯Ø±", "Ø¨Ø±Ø§Ø¯Ø±", "Ø®ÙˆØ§Ù‡Ø±", "Ù‡Ù…Ø³Ø±", "ÙØ±Ø²Ù†Ø¯"),
        ("ØªÙˆ {rel} Ù…Ù†ÛŒ", "Ù¾Ø¯Ø±", "Ù…Ø§Ø¯Ø±", "Ø¨Ø±Ø§Ø¯Ø±", "Ø®ÙˆØ§Ù‡Ø±", "Ù‡Ù…Ø³Ø±", "ÙØ±Ø²Ù†Ø¯"),
        ("Ù…ÛŒâ€ŒØ®ÙˆØ§Ù… Ø¨Ø§ {rel}Ù… ØµØ­Ø¨Øª Ú©Ù†Ù…", "Ù¾Ø¯Ø±", "Ù…Ø§Ø¯Ø±", "Ø¨Ø±Ø§Ø¯Ø±", "Ø®ÙˆØ§Ù‡Ø±", "Ù‡Ù…Ø³Ø±", "ÙØ±Ø²Ù†Ø¯"),
        ("ÛŒØ§Ø¯Øª Ú†Ù‚Ø¯Ø± {action} Ù…ÛŒâ€ŒÚ©Ø±Ø¯ÛŒÙ…ØŸ", "Ø´ÛŒØ·Ù†Øª", "Ø¨Ø§Ø²ÛŒ", "Ø®ÙˆØ´ Ù…ÛŒâ€ŒÚ¯Ø°Ø±ÙˆÙ†Ø¯ÛŒÙ…", "Ø­Ø±Ù Ù…ÛŒâ€ŒØ²Ø¯ÛŒÙ…", "Ø®Ù†Ø¯ÛŒØ¯ÛŒÙ…"),
        ("Ø²Ù†Ø¯Ú¯ÛŒ Ø®ÙˆØ¨ÛŒ Ø¯Ø§Ø±ÛŒØŸ", None),
        ("ØªÙˆ Ú†Ø·ÙˆØ±ÛŒ Ø²Ù†Ø¯Ú¯ÛŒ Ø¯Ø§Ø±ÛŒØŸ", None),
        ("Ù…ÛŒâ€ŒØªÙˆÙ†ÛŒ Ø¯Ø± Ù…ÙˆØ±Ø¯ Ø®ÙˆØ¯Øª Ø¨Ú¯ÛŒØŸ", None),
        ("Ù…ÛŒâ€ŒØªÙˆÙ†ÛŒ Ø¯Ø± Ù…ÙˆØ±Ø¯ Ø²Ù†Ø¯Ú¯ÛŒ Ø¨Ù‡Ù… Ø¨Ú¯ÛŒØŸ", None),
        ("Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ù…Ø±Ú¯ Ùˆ Ø²Ù†Ø¯Ú¯ÛŒ Ù¾Ø³ Ø§Ø² Ø¢Ù† ØªÙˆØ¶ÛŒØ­ Ø¨Ø¯Ù‡", None),
        ("Ø¨Ù‡Ø´Øª Ú†Ú¯ÙˆÙ†Ù‡ Ø¬Ø§ÛŒÛŒ Ø§Ø³ØªØŸ", None),
        ("Ú†Ú¯ÙˆÙ†Ù‡ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù… Ø¢Ø±Ø§Ù…Ø´ Ù…Ø¹Ù†ÙˆÛŒ Ù¾ÛŒØ¯Ø§ Ú©Ù†Ù…ØŸ", None),
    ]
    
    years = ["Û¹Ûµ", "Û¹Û¶", "Û¹Û·", "Û¹Û¸", "Û¹Û¹", "Û±Û´Û°Û°", "Û±Û´Û°Û±", "Û±Û´Û°Û²", "Û±Û´Û°Û³"]
    actions = ["Ø´ÛŒØ·Ù†Øª", "Ø¨Ø§Ø²ÛŒ", "Ø®ÙˆØ´ Ù…ÛŒâ€ŒÚ¯Ø°Ø±ÙˆÙ†Ø¯ÛŒÙ…", "Ø­Ø±Ù Ù…ÛŒâ€ŒØ²Ø¯ÛŒÙ…", "Ø®Ù†Ø¯ÛŒØ¯ÛŒÙ…", "Ø³ÙØ± Ù…ÛŒâ€ŒÚ©Ø±Ø¯ÛŒÙ…"]
    
    device = next(model.parameters()).device
    
    generated = 0
    attempts = 0
    max_attempts = num_examples * 3  # Ø­Ø¯Ø§Ú©Ø«Ø± ØªÙ„Ø§Ø´
    
    while generated < num_examples and attempts < max_attempts:
        attempts += 1
        
        # Ø§Ù†ØªØ®Ø§Ø¨ Ø§Ù„Ú¯Ùˆ
        pattern_template, *rel_options = random.choice(patterns)
        
        # Ø³Ø§Ø®Øª instruction
        if "{rel}" in pattern_template:
            if rel_options and rel_options[0]:
                rel = random.choice(rel_options)
                if "{year}" in pattern_template:
                    year = random.choice(years)
                    instruction = pattern_template.format(rel=rel, year=year)
                else:
                    instruction = pattern_template.format(rel=rel)
            else:
                continue
        elif "{action}" in pattern_template:
            action = random.choice(actions)
            instruction = pattern_template.format(action=action)
        else:
            instruction = pattern_template
        
        # ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®
        prompt = f"User: {instruction}\nAssistant:"
        inputs = tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=256
        ).to(device)
        
        try:
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=250,
                    temperature=0.8,
                    top_p=0.9,
                    repetition_penalty=1.2,
                    do_sample=True,
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                    num_return_sequences=1,
                )
            
            input_length = inputs["input_ids"].shape[1]
            response = tokenizer.decode(outputs[0][input_length:], skip_special_tokens=True).strip()
            
            # ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ø®ÙˆØ¨
            if (response and 
                len(response) > 30 and 
                len(response) < 600 and
                not response.startswith("User:") and
                not response.startswith("Assistant:") and
                any(ord(c) > 127 for c in response)):  # Ø­ØªÙ…Ø§Ù‹ ÙØ§Ø±Ø³ÛŒ Ø¨Ø§Ø´Ø¯
                
                new_examples.append({
                    "instruction": instruction,
                    "response": response
                })
                generated += 1
                
                if generated % 10 == 0:
                    print(f"  âœ“ Generated {generated}/{num_examples} examples...")
        
        except Exception as e:
            continue
    
    print(f"âœ… Generated {len(new_examples)} valid examples")
    return new_examples

def train_model(dataset):
    """Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„"""
    print(f"\nğŸ“ Starting training with {len(dataset)} examples...")
    
    # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ ÙØ±Ù…Øª Ù…Ù†Ø§Ø³Ø¨
    def format_prompt(example):
        instruction = example.get("instruction", "")
        response = example.get("response", "")
        prompt = f"User: {instruction}\nAssistant: {response}"
        return {"text": prompt}
    
    formatted_data = [format_prompt(ex) for ex in dataset]
    train_dataset = Dataset.from_list(formatted_data)
    
    # ØªÙ‚Ø³ÛŒÙ… train/eval
    train_dataset = train_dataset.train_test_split(test_size=0.1, seed=42)
    eval_dataset = train_dataset["test"]
    train_dataset = train_dataset["train"]
    
    print(f"ğŸ“Š Split: {len(train_dataset)} train, {len(eval_dataset)} eval")
    
    # Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id
    
    # Quantization
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
    )
    
    os.environ["HF_HUB_DOWNLOAD_TIMEOUT"] = "300"
    
    # Load model
    if os.path.exists(FINAL_MODEL_DIR) and os.path.exists(f"{FINAL_MODEL_DIR}/adapter_config.json"):
        print("ğŸ“¥ Loading existing model...")
        base_model = AutoModelForCausalLM.from_pretrained(
            BASE_MODEL,
            quantization_config=bnb_config,
            trust_remote_code=True,
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True,
            device_map=None,
        )
        model = PeftModel.from_pretrained(base_model, FINAL_MODEL_DIR)
        model = prepare_model_for_kbit_training(model)
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ùˆ ÙØ¹Ø§Ù„ Ú©Ø±Ø¯Ù† trainable parameters
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        if trainable_params == 0:
            print("âš ï¸ Warning: No trainable parameters! Enabling training for LoRA layers...")
            # ÙØ¹Ø§Ù„ Ú©Ø±Ø¯Ù† training Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ LoRA parameters
            for name, param in model.named_parameters():
                if 'lora' in name.lower() or 'adapter' in name.lower():
                    param.requires_grad = True
            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            print(f"âœ… Enabled {trainable_params:,} trainable parameters")
        
        model.train()
    else:
        print("ğŸ“¥ Loading base model...")
        model = AutoModelForCausalLM.from_pretrained(
            BASE_MODEL,
            quantization_config=bnb_config,
            trust_remote_code=True,
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True,
            device_map=None,
        )
        model = prepare_model_for_kbit_training(model)
        
        lora_config = LoraConfig(
            r=16,
            lora_alpha=32,
            target_modules=["c_attn", "c_proj", "c_fc"],
            lora_dropout=0.05,
            bias="none",
            task_type="CAUSAL_LM",
        )
        model = get_peft_model(model, lora_config)
    
    # Ø¨Ø±Ø±Ø³ÛŒ Ù†Ù‡Ø§ÛŒÛŒ trainable parameters
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    print(f"Final check - Trainable params: {trainable_params:,} || Total params: {total_params:,} || Trainable%: {100 * trainable_params / total_params:.4f}")
    
    if trainable_params == 0:
        raise ValueError("No trainable parameters found! Cannot train model.")
    
    # Tokenize
    def tokenize_function(examples):
        tokenized = tokenizer(
            examples["text"],
            truncation=True,
            max_length=512,
            padding="max_length",
            return_tensors=None,
        )
        
        labels = []
        for i, text in enumerate(examples["text"]):
            assistant_prefix = "Assistant:"
            assistant_start_idx = text.find(assistant_prefix)
            
            if assistant_start_idx != -1:
                prompt_tokens = tokenizer(
                    text[:assistant_start_idx + len(assistant_prefix)],
                    truncation=True,
                    max_length=512,
                    padding=False,
                    return_tensors=None,
                )["input_ids"]
                prompt_length = len(prompt_tokens)
            else:
                prompt_length = len(tokenized["input_ids"][i]) // 2
            
            current_labels = list(tokenized["input_ids"][i])
            for j in range(min(prompt_length, len(current_labels))):
                current_labels[j] = -100
            labels.append(current_labels)
        
        tokenized["labels"] = labels
        return tokenized
    
    print("ğŸ”¤ Tokenizing...")
    train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=["text"])
    eval_dataset = eval_dataset.map(tokenize_function, batched=True, remove_columns=["text"])
    
    # Training args
    training_args = TrainingArguments(
        output_dir="./checkpoints",
        num_train_epochs=2,
        per_device_train_batch_size=4,
        per_device_eval_batch_size=4,
        gradient_accumulation_steps=2,
        warmup_steps=50,
        logging_steps=20,
        eval_strategy="steps",
        eval_steps=100,
        save_steps=200,
        save_total_limit=2,
        learning_rate=2e-4,
        fp16=False,  # ØºÛŒØ±ÙØ¹Ø§Ù„ Ú©Ø±Ø¯Ù† fp16 Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ù…Ø´Ú©Ù„ optimizer
        bf16=False,
        optim="paged_adamw_8bit",
        report_to="none",
        load_best_model_at_end=True,
        metric_for_best_model="loss",
        greater_is_better=False,
        remove_unused_columns=False,
        dataloader_pin_memory=False,
    )
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        tokenizer=tokenizer,
    )
    
    print("ğŸš€ Training started...")
    trainer.train()
    
    # Save
    print("ğŸ’¾ Saving model...")
    model.save_pretrained(FINAL_MODEL_DIR)
    tokenizer.save_pretrained(FINAL_MODEL_DIR)
    
    print("âœ… Training completed!")
    return model, tokenizer

def main_loop():
    """Ø­Ù„Ù‚Ù‡ Ø§ØµÙ„ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù¾ÛŒÙˆØ³ØªÙ‡"""
    cycle = 0
    
    while True:
        cycle += 1
        print("\n" + "=" * 60)
        print(f"ğŸ”„ CYCLE {cycle} - {time.strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 60)
        
        # 1. Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ dataset
        dataset = load_dataset()
        
        # 2. Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ
        backup_dataset(dataset)
        
        # 3. Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯
        if os.path.exists(FINAL_MODEL_DIR) and os.path.exists(f"{FINAL_MODEL_DIR}/adapter_config.json"):
            print("\nğŸ“¥ Loading model for generation...")
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True,
            )
            
            base_model = AutoModelForCausalLM.from_pretrained(
                BASE_MODEL,
                quantization_config=bnb_config,
                trust_remote_code=True,
                torch_dtype=torch.float16,
                device_map=None,
            )
            model = PeftModel.from_pretrained(base_model, FINAL_MODEL_DIR)
            tokenizer = AutoTokenizer.from_pretrained(FINAL_MODEL_DIR, trust_remote_code=True)
            
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            # 4. ØªÙˆÙ„ÛŒØ¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯
            new_examples = generate_new_examples(model, tokenizer, GENERATE_NEW_EXAMPLES)
            
            # 5. Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¨Ù‡ dataset
            if new_examples:
                dataset.extend(new_examples)
                
                # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ø§Ù†Ø¯Ø§Ø²Ù‡ dataset
                if len(dataset) > MAX_DATASET_SIZE:
                    print(f"ğŸ“‰ Dataset too large ({len(dataset)}), keeping last {MAX_DATASET_SIZE} examples")
                    dataset = dataset[-MAX_DATASET_SIZE:]
                
                # Ø°Ø®ÛŒØ±Ù‡ dataset
                with open(DATASET_FILE, "w", encoding="utf-8") as f:
                    json.dump(dataset, f, ensure_ascii=False, indent=2)
                print(f"ğŸ“š Dataset updated: {len(dataset)} examples")
            
            # Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† Ù…Ø¯Ù„ Ø§Ø² Ø­Ø§ÙØ¸Ù‡
            del model
            del base_model
            torch.cuda.empty_cache()
        
        # 6. Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„
        model, tokenizer = train_model(dataset)
        
        # Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† Ø§Ø² Ø­Ø§ÙØ¸Ù‡
        del model
        torch.cuda.empty_cache()
        
        # 7. Ø§Ù†ØªØ¸Ø§Ø± Ù‚Ø¨Ù„ Ø§Ø² Ú†Ø±Ø®Ù‡ Ø¨Ø¹Ø¯ÛŒ
        print(f"\nâ³ Waiting {MIN_CYCLE_INTERVAL} seconds before next cycle...")
        print("   (Press Ctrl+C to stop)")
        time.sleep(MIN_CYCLE_INTERVAL)

if __name__ == "__main__":
    try:
        main_loop()
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ Continuous learning stopped by user")
        print("âœ… All progress saved!")
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


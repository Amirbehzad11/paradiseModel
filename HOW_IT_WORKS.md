# ساز و کار سیستم - How It Works

## 🎯 نمای کلی سیستم
## System Overview

این سیستم یک **چت بات فارسی** است که به عنوان **روح عزیزان فوت‌شده** صحبت می‌کند.

## 📊 معماری سیستم
## System Architecture

```
┌─────────────────────────────────────────────────────────┐
│                    مدل پایه (Base Model)                  │
│              HooshvareLab/gpt2-fa (124M)                │
│              مدل GPT2 فارسی از Hugging Face             │
└──────────────────────┬──────────────────────────────────┘
                       │
                       ↓
┌─────────────────────────────────────────────────────────┐
│              Quantization (4-bit QLoRA)                 │
│  - کاهش حافظه از 500MB به ~150MB                        │
│  - امکان train روی GPU های 6GB+                         │
└──────────────────────┬──────────────────────────────────┘
                       │
                       ↓
┌─────────────────────────────────────────────────────────┐
│              LoRA Adapter (Fine-tuning)                 │
│  - فقط 8.9M پارامتر trainable                          │
│  - آموزش روی dataset فارسی                              │
│  - یادگیری سبک صحبت به عنوان روح عزیزان                │
└──────────────────────┬──────────────────────────────────┘
                       │
                       ↓
┌─────────────────────────────────────────────────────────┐
│              Final Model (./final_model)                 │
│  - Base Model + LoRA Adapter                            │
│  - آماده برای چت                                        │
└─────────────────────────────────────────────────────────┘
```

## 🔄 سه حالت اجرا
## Three Execution Modes

### 1️⃣ حالت عادی (chat.py)
```
کاربر → مدل → پاسخ
```

**ویژگی‌ها:**
- فقط چت می‌کند
- یادگیری ندارد
- سریع و ساده

**استفاده:**
```bash
python chat.py
```

---

### 2️⃣ حالت یادگیری از چت (chat_with_learning.py)
```
کاربر → مدل → پاسخ
         ↓
    ذخیره در new_chats.json
         ↓
    بعد از 10 چت:
    train_incremental.py
         ↓
    اضافه به dataset.json
         ↓
    Retrain مدل
         ↓
    مدل بهتر می‌شود
```

**ویژگی‌ها:**
- از چت‌های شما یاد می‌گیرد
- بعد از هر 10 چت، retrain می‌شود
- Dataset بزرگتر می‌شود
- مدل بهتر می‌شود

**استفاده:**
```bash
python chat_with_learning.py
```

**چرخه یادگیری:**
1. شما با مدل چت می‌کنید
2. هر چت در `new_chats.json` ذخیره می‌شود
3. بعد از 10 چت → `train_incremental.py` اجرا می‌شود
4. چت‌های جدید به `dataset.json` اضافه می‌شوند
5. مدل retrain می‌شود (2 epochs)
6. مدل جدید لود می‌شود
7. ادامه چت با مدل بهتر

---

### 3️⃣ حالت یادگیری پیوسته (continuous_train.py)
```
┌─────────────────────────────────────────┐
│  چرخه 1:                                │
│  1. بارگذاری dataset                    │
│  2. بارگذاری model                      │
│  3. تولید 100 نمونه جدید از model      │
│  4. اضافه به dataset                    │
│  5. Train model (2 epochs)              │
│  6. ذخیره model                        │
│  7. انتظار 5 دقیقه                     │
└──────────────┬──────────────────────────┘
               ↓
┌─────────────────────────────────────────┐
│  چرخه 2: (با model بهبود یافته)        │
│  1. بارگذاری dataset (بزرگتر)          │
│  2. بارگذاری model (بهتر)              │
│  3. تولید 100 نمونه جدید (بهتر)        │
│  4. اضافه به dataset                    │
│  5. Train model                         │
│  6. ذخیره model                        │
│  7. انتظار 5 دقیقه                     │
└──────────────┬──────────────────────────┘
               ↓
         (ادامه بی‌نهایت)
```

**ویژگی‌ها:**
- همیشه در حال train است
- خودش داده تولید می‌کند
- Dataset تا 10000 نمونه رشد می‌کند
- مدل مداوم بهتر می‌شود
- برای سرورهای 24/7

**استفاده:**
```bash
python continuous_train.py
```

**چرخه:**
- هر 5 دقیقه یک چرخه کامل
- تولید 100 نمونه جدید
- Train 2 epochs
- Dataset بزرگتر می‌شود

---

## 📚 Dataset
## Dataset Structure

### فرمت:
```json
[
  {
    "instruction": "این عکس پدرمه که سال ۹۸ فوت کرد. می‌خوام باهاش حرف بزنم",
    "response": "سلام پسرم… من اینجام. همیشه پیشتم. دلت تنگ شده؟..."
  },
  ...
]
```

### محتوا:
- **نمونه‌های عمومی**: درباره مرگ، بهشت، زندگی پس از مرگ
- **نمونه‌های اول شخص**: پدر، مادر، برادر، خواهر، همسر، فرزند
- **سبک**: گرم، احساسی، واقعی، فارسی 100%

### رشد Dataset:
- **شروع**: 3000+ نمونه
- **با chat_with_learning**: +10 نمونه هر 10 چت
- **با continuous_train**: +100 نمونه هر 5 دقیقه
- **حداکثر**: 10000 نمونه

---

## 🧠 فرآیند Training
## Training Process

### 1. آماده‌سازی داده:
```
dataset.json → format_prompt → "User: {instruction}\nAssistant: {response}"
```

### 2. Tokenization:
```
متن → Token IDs → Labels (با masking prompt)
```

**نکته مهم**: فقط بخش "Assistant:" train می‌شود، prompt mask می‌شود.

### 3. Quantization:
```
Base Model → 4-bit Quantization → 75% کاهش حافظه
```

### 4. LoRA:
```
Base Model → LoRA Adapter → فقط 8.9M پارامتر trainable
```

### 5. Training:
```
- 2-3 epochs
- Learning rate: 2e-4
- Batch size: 4
- Optimizer: paged_adamw_8bit
```

### 6. ذخیره:
```
./final_model/
  ├── adapter_config.json
  ├── adapter_model.bin
  └── tokenizer files
```

---

## 💬 فرآیند Inference (چت)
## Inference Process

### 1. بارگذاری:
```
Base Model (4-bit) + LoRA Adapter → PeftModel
```

### 2. ساخت Prompt:
```
"User: {user_input}\nAssistant:"
```

### 3. Generation:
```
- Temperature: 0.7
- Top-p: 0.9
- Repetition penalty: 1.2
- Max tokens: 300
```

### 4. خروجی:
```
فقط بخش "Assistant:" را برمی‌گرداند
```

---

## 🔧 فایل‌های کلیدی
## Key Files

| فایل | کاربرد |
|------|--------|
| `train_once.py` | آموزش اولیه (یک بار) |
| `chat.py` | چت عادی (بدون یادگیری) |
| `chat_with_learning.py` | چت با یادگیری از کاربر |
| `train_incremental.py` | آموزش تدریجی (فراخوانی خودکار) |
| `continuous_train.py` | یادگیری پیوسته (24/7) |
| `dataset.json` | Dataset اصلی |
| `new_chats.json` | چت‌های جدید برای یادگیری |
| `./final_model/` | مدل نهایی |

---

## 📈 پیشرفت و بهبود
## Progress and Improvement

### با chat_with_learning.py:
```
چت 1-10:   Dataset: 3000 → 3010
چت 11-20:  Dataset: 3010 → 3020
چت 21-30:  Dataset: 3020 → 3030
...
مدل بهتر می‌شود چون از چت‌های واقعی شما یاد می‌گیرد
```

### با continuous_train.py:
```
چرخه 1:  Dataset: 3000 → 3100 (100 نمونه جدید)
چرخه 2:  Dataset: 3100 → 3200 (100 نمونه جدید)
چرخه 3:  Dataset: 3200 → 3300 (100 نمونه جدید)
...
مدل از خودش یاد می‌گیرد و بهتر می‌شود
```

---

## 🎯 خلاصه
## Summary

1. **مدل پایه**: GPT2 فارسی (HooshvareLab/gpt2-fa)
2. **Quantization**: 4-bit برای کاهش حافظه
3. **Fine-tuning**: LoRA برای آموزش سریع
4. **Dataset**: فارسی، درباره روح عزیزان فوت‌شده
5. **یادگیری**: 
   - از چت‌های کاربر (chat_with_learning)
   - یا خودکار (continuous_train)
6. **نتیجه**: مدل که به عنوان روح عزیزان صحبت می‌کند

---

## 💡 نکات مهم
## Important Notes

- **اولین بار**: باید `train_once.py` را اجرا کنید
- **حافظه**: حداقل 6GB VRAM لازم است
- **زمان**: Training اولیه 30-40 دقیقه
- **بهبود**: هر چه بیشتر استفاده کنید، بهتر می‌شود
- **آفلاین**: بعد از اولین download، کاملاً آفلاین کار می‌کند

---

**پیشنهاد**: برای شروع از `chat_with_learning.py` استفاده کنید تا مدل از چت‌های شما یاد بگیرد!


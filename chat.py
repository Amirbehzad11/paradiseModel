#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ú†Øª Ø¨Ø§Øª ØªØ±Ù…ÛŒÙ†Ø§Ù„ÛŒ - Ø­Ù„Ù‚Ù‡ Ø¨ÛŒâ€ŒÙ†Ù‡Ø§ÛŒØª
Terminal chat bot - infinite loop
"""

import os
import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig
)
from peft import PeftModel
import sys

# Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ù…Ø¯Ù„
# Check if model exists
if not os.path.exists("./final_model"):
    print("âŒ Ø®Ø·Ø§: Ù…Ø¯Ù„ Ø¢Ù…ÙˆØ²Ø´â€ŒØ¯ÛŒØ¯Ù‡ ÛŒØ§ÙØª Ù†Ø´Ø¯!")
    print("âŒ Error: Trained model not found!")
    print("ğŸ“ Ù„Ø·ÙØ§ Ø§Ø¨ØªØ¯Ø§ train_once.py Ø±Ø§ Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯")
    print("ğŸ“ Please run train_once.py first")
    sys.exit(1)

print("ğŸ¤– Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„...")
print("ğŸ¤– Loading model...")

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª quantization
# Quantization settings
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
)

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ù¾Ø§ÛŒÙ‡
# Load base model
BASE_MODEL = "meta-llama/Llama-3.2-1B-Instruct"  # Ø¨Ø§ÛŒØ¯ Ø¨Ø§ train_once.py ÛŒÚ©Ø³Ø§Ù† Ø¨Ø§Ø´Ø¯
hf_token = os.getenv("HF_TOKEN") or os.getenv("HUGGINGFACE_TOKEN")

print("ğŸ“¥ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ù¾Ø§ÛŒÙ‡...")
print("ğŸ“¥ Loading base model...")
base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    quantization_config=bnb_config,
    device_map="auto",
    token=hf_token,
    trust_remote_code=True,
    torch_dtype=torch.float16,
)

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ LoRA weights
# Load LoRA weights
print("ğŸ“¥ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ LoRA...")
print("ğŸ“¥ Loading LoRA weights...")
model = PeftModel.from_pretrained(base_model, "./final_model")
model = model.merge_and_unload()  # Ø§Ø¯ØºØ§Ù… LoRA Ø¨Ø§ Ù…Ø¯Ù„ Ù¾Ø§ÛŒÙ‡ Ø¨Ø±Ø§ÛŒ Ø³Ø±Ø¹Øª Ø¨ÛŒØ´ØªØ±

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø±
# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained("./final_model", trust_remote_code=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.pad_token_id = tokenizer.eos_token_id

print("âœ… Ù…Ø¯Ù„ Ø¢Ù…Ø§Ø¯Ù‡ Ø§Ø³Øª!")
print("âœ… Model ready!")
print("=" * 50)
print("ğŸ’¬ Ú†Øª Ø¨Ø§Øª Ø¢Ù…Ø§Ø¯Ù‡ Ø§Ø³Øª. Ø¨Ø±Ø§ÛŒ Ø®Ø±ÙˆØ¬ 'Ø®Ø±ÙˆØ¬' ÛŒØ§ 'exit' ØªØ§ÛŒÙ¾ Ú©Ù†ÛŒØ¯")
print("ğŸ’¬ Chat bot ready. Type 'Ø®Ø±ÙˆØ¬' or 'exit' to quit")
print("=" * 50)

# Ø­Ù„Ù‚Ù‡ Ú†Øª Ø¨ÛŒâ€ŒÙ†Ù‡Ø§ÛŒØª
# Infinite chat loop
while True:
    try:
        # Ø¯Ø±ÛŒØ§ÙØª ÙˆØ±ÙˆØ¯ÛŒ Ú©Ø§Ø±Ø¨Ø±
        # Get user input
        user_input = input("\nğŸ‘¤ Ø´Ù…Ø§: ").strip()
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø®Ø±ÙˆØ¬
        # Check exit
        if user_input.lower() in ["Ø®Ø±ÙˆØ¬", "exit", "quit", "q"]:
            print("\nğŸ‘‹ Ø®Ø¯Ø§Ø­Ø§ÙØ¸!")
            print("ğŸ‘‹ Goodbye!")
            break
        
        if not user_input:
            continue
        
        # ÙØ±Ù…Øª Ú©Ø±Ø¯Ù† prompt
        # Format prompt
        prompt = f"### Instruction:\n{user_input}\n\n### Response:\n"
        
        # ØªÙˆÚ©Ù†Ø§ÛŒØ² Ú©Ø±Ø¯Ù†
        # Tokenize
        inputs = tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=512
        ).to(model.device)
        
        # ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®
        # Generate response
        print("ğŸ¤– Ù…Ø¯Ù„: ", end="", flush=True)
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )
        
        # Ø¯ÛŒÚ©Ø¯ Ú©Ø±Ø¯Ù† Ù¾Ø§Ø³Ø®
        # Decode response
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙÙ‚Ø· Ø¨Ø®Ø´ Ù¾Ø§Ø³Ø®
        # Extract only response part
        if "### Response:" in response:
            response = response.split("### Response:")[-1].strip()
        else:
            # Ø§Ú¯Ø± ÙØ±Ù…Øª Ù†Ø¨ÙˆØ¯ØŒ Ú©Ù„ Ù¾Ø§Ø³Ø® Ø±Ø§ Ø¨Ú¯ÛŒØ±
            # If format not found, take full response
            response = response[len(prompt):].strip()
        
        print(response)
        
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ Ø®Ø¯Ø§Ø­Ø§ÙØ¸!")
        print("ğŸ‘‹ Goodbye!")
        break
    except Exception as e:
        print(f"\nâŒ Ø®Ø·Ø§: {e}")
        print(f"âŒ Error: {e}")
        continue


#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ú†Øª Ø¨Ø§Øª ØªØ±Ù…ÛŒÙ†Ø§Ù„ÛŒ - Ø­Ù„Ù‚Ù‡ Ø¨ÛŒâ€ŒÙ†Ù‡Ø§ÛŒØª
Terminal chat bot - infinite loop
"""

import os
import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig
)
from peft import PeftModel
from huggingface_hub import HfFolder
import sys

# Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ù…Ø¯Ù„
# Check if model exists
if not os.path.exists("./final_model"):
    print("âŒ Ø®Ø·Ø§: Ù…Ø¯Ù„ Ø¢Ù…ÙˆØ²Ø´â€ŒØ¯ÛŒØ¯Ù‡ ÛŒØ§ÙØª Ù†Ø´Ø¯!")
    print("âŒ Error: Trained model not found!")
    print("ğŸ“ Ù„Ø·ÙØ§ Ø§Ø¨ØªØ¯Ø§ train_once.py Ø±Ø§ Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯")
    print("ğŸ“ Please run train_once.py first")
    sys.exit(1)

print("ğŸ¤– Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„...")
print("ğŸ¤– Loading model...")

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª quantization
# Quantization settings
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
)

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ù¾Ø§ÛŒÙ‡ - Ø¨Ø§ÛŒØ¯ Ø¨Ø§ train_once.py ÛŒÚ©Ø³Ø§Ù† Ø¨Ø§Ø´Ø¯
# Load base model - must match train_once.py
BASE_MODEL = "microsoft/Phi-3-mini-4k-instruct"  # Ú©Ø§Ù…Ù„Ø§Ù‹ Ø¨Ø§Ø²ØŒ Ø¨Ø¯ÙˆÙ† Ù…Ø¬ÙˆØ²
# BASE_MODEL = "Qwen/Qwen2-1.5B-Instruct"  # Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†: Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø¹Ø§Ù„ÛŒ Ø§Ø² ÙØ§Ø±Ø³ÛŒ

# Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø²ØŒ ØªÙˆÚ©Ù† Ø§Ø®ØªÛŒØ§Ø±ÛŒ Ø§Ø³Øª
# For open models, token is optional
from huggingface_hub import HfFolder
hf_token = (
    os.getenv("HF_TOKEN") or 
    os.getenv("HUGGINGFACE_TOKEN") or
    HfFolder.get_token()
)

print("ğŸ“¥ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ù¾Ø§ÛŒÙ‡...")
print("ğŸ“¥ Loading base model...")
base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    quantization_config=bnb_config,
    device_map="auto",
    token=hf_token,
    trust_remote_code=True,
    torch_dtype=torch.float16,
)

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ LoRA weights
# Load LoRA weights
print("ğŸ“¥ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ LoRA...")
print("ğŸ“¥ Loading LoRA weights...")
model = PeftModel.from_pretrained(base_model, "./final_model")
# Ø¨Ø±Ø§ÛŒ 4-bit quantizationØŒ merge Ù…Ù…Ú©Ù† Ø§Ø³Øª Ù…Ø´Ú©Ù„Ø§ØªÛŒ Ø§ÛŒØ¬Ø§Ø¯ Ú©Ù†Ø¯
# For 4-bit quantization, merge may cause issues
# Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…Ø³ØªÙ‚ÛŒÙ… Ø§Ø² LoRA Ø¨Ø¯ÙˆÙ† merge (Ø¨Ù‡ØªØ± Ø¨Ø±Ø§ÛŒ 4-bit)
# Use LoRA directly without merge (better for 4-bit)
print("âœ… LoRA weights loaded (using without merge for better compatibility)")

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø±
# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained("./final_model", trust_remote_code=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.pad_token_id = tokenizer.eos_token_id

print("âœ… Ù…Ø¯Ù„ Ø¢Ù…Ø§Ø¯Ù‡ Ø§Ø³Øª!")
print("âœ… Model ready!")
print("=" * 50)
print("ğŸ’¬ Ú†Øª Ø¨Ø§Øª Ø¢Ù…Ø§Ø¯Ù‡ Ø§Ø³Øª. Ø¨Ø±Ø§ÛŒ Ø®Ø±ÙˆØ¬ 'Ø®Ø±ÙˆØ¬' ÛŒØ§ 'exit' ØªØ§ÛŒÙ¾ Ú©Ù†ÛŒØ¯")
print("ğŸ’¬ Chat bot ready. Type 'Ø®Ø±ÙˆØ¬' or 'exit' to quit")
print("=" * 50)

# Ø­Ù„Ù‚Ù‡ Ú†Øª Ø¨ÛŒâ€ŒÙ†Ù‡Ø§ÛŒØª
# Infinite chat loop
while True:
    try:
        # Ø¯Ø±ÛŒØ§ÙØª ÙˆØ±ÙˆØ¯ÛŒ Ú©Ø§Ø±Ø¨Ø±
        # Get user input
        user_input = input("\nğŸ‘¤ Ø´Ù…Ø§: ").strip()
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø®Ø±ÙˆØ¬
        # Check exit
        if user_input.lower() in ["Ø®Ø±ÙˆØ¬", "exit", "quit", "q"]:
            print("\nğŸ‘‹ Ø®Ø¯Ø§Ø­Ø§ÙØ¸!")
            print("ğŸ‘‹ Goodbye!")
            break
        
        if not user_input:
            continue
        
        # ÙØ±Ù…Øª Ú©Ø±Ø¯Ù† prompt (ÙØ±Ù…Øª Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ instruction - Ù‡Ù…Ø§Ù† ÙØ±Ù…Øª Ø¢Ù…ÙˆØ²Ø´)
        # Format prompt (standard instruction format - same as training)
        prompt = f"### Instruction:\n{user_input}\n\n### Response:\n"
        
        # ØªÙˆÚ©Ù†Ø§ÛŒØ² Ú©Ø±Ø¯Ù†
        # Tokenize
        inputs = tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=512
        ).to(model.device)
        
        # ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®
        # Generate response
        print("ğŸ¤– Ù…Ø¯Ù„: ", end="", flush=True)
        with torch.no_grad():
            try:
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=128,  # Ú©Ø§Ù‡Ø´ Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ú©ÙˆØªØ§Ù‡â€ŒØªØ±
                    temperature=0.3,  # Ú©Ø§Ù‡Ø´ Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ±
                    top_p=0.85,
                    do_sample=True,
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                    repetition_penalty=1.2,  # Ø§ÙØ²Ø§ÛŒØ´ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ØªÚ©Ø±Ø§Ø±
                    no_repeat_ngram_size=3,  # Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ØªÚ©Ø±Ø§Ø± n-gram
                )
            except Exception as e:
                print(f"\nâŒ Ø®Ø·Ø§ Ø¯Ø± ØªÙˆÙ„ÛŒØ¯: {e}")
                print("âŒ Error in generation: {e}")
                continue
        
        # Ø¯ÛŒÚ©Ø¯ Ú©Ø±Ø¯Ù† Ù¾Ø§Ø³Ø®
        # Decode response
        if len(outputs) == 0 or len(outputs[0]) == 0:
            print("(Ù¾Ø§Ø³Ø® Ø®Ø§Ù„ÛŒ)")
            print("(Empty response)")
            continue
            
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙÙ‚Ø· Ø¨Ø®Ø´ Ù¾Ø§Ø³Ø®
        # Extract only response part
        if "### Response:" in response:
            response = response.split("### Response:")[-1].strip()
        else:
            # Ø§Ú¯Ø± ÙØ±Ù…Øª Ù†Ø¨ÙˆØ¯ØŒ ÙÙ‚Ø· Ø¨Ø®Ø´ Ø¬Ø¯ÛŒØ¯ Ø±Ø§ Ø¨Ú¯ÛŒØ± (Ø¨Ø¹Ø¯ Ø§Ø² prompt)
            # If format not found, take only new part (after prompt)
            input_length = inputs["input_ids"].shape[1]
            response = tokenizer.decode(outputs[0][input_length:], skip_special_tokens=True).strip()
        
        if not response:
            print("(Ù¾Ø§Ø³Ø® Ø®Ø§Ù„ÛŒ)")
            print("(Empty response)")
        else:
            print(response)
        
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ Ø®Ø¯Ø§Ø­Ø§ÙØ¸!")
        print("ğŸ‘‹ Goodbye!")
        break
    except Exception as e:
        print(f"\nâŒ Ø®Ø·Ø§: {e}")
        print(f"âŒ Error: {e}")
        continue


#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
API Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø®Ø§Ø±Ø¬ÛŒ Ø§Ø² Ù…Ø¯Ù„ Ú†Øª
REST API for external use of chat model
"""
import os
import sys
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel
import warnings
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn

warnings.filterwarnings("ignore")
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# Ø§ÛŒØ¬Ø§Ø¯ FastAPI app
app = FastAPI(
    title="Spiritual Chatbot API",
    description="API Ø¨Ø±Ø§ÛŒ Ú†Øª Ø¨Ø§ Ù…Ø¯Ù„ Ø±ÙˆØ­ Ø¹Ø²ÛŒØ²Ø§Ù† ÙÙˆØªâ€ŒØ´Ø¯Ù‡",
    version="1.0.0"
)

# ÙØ¹Ø§Ù„ Ú©Ø±Ø¯Ù† CORS Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Ø¯Ø± production Ø¨Ù‡ØªØ± Ø§Ø³Øª Ù…Ø­Ø¯ÙˆØ¯ Ø´ÙˆØ¯
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ global
peft_model = None
tokenizer = None
BASE_MODEL = "HooshvareLab/gpt2-fa"

# Schema Ø¨Ø±Ø§ÛŒ request Ùˆ response
class ChatRequest(BaseModel):
    message: str
    max_tokens: int = 300
    temperature: float = 0.7
    top_p: float = 0.9
    repetition_penalty: float = 1.2

class ChatResponse(BaseModel):
    response: str
    status: str = "success"

class HealthResponse(BaseModel):
    status: str
    model_loaded: bool

def load_model():
    """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„"""
    global peft_model, tokenizer
    
    if not os.path.exists("./final_model"):
        raise FileNotFoundError("Model not found. Please run train_once.py first.")
    
    print("Loading model...")
    
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
    )
    
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True,
        torch_dtype=torch.float16,
        attn_implementation="eager",
    )
    
    peft_model = PeftModel.from_pretrained(base_model, "./final_model")
    tokenizer = AutoTokenizer.from_pretrained("./final_model", trust_remote_code=True)
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id
    
    print("âœ… Model loaded successfully!")

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ø¯Ø± startup
@app.on_event("startup")
async def startup_event():
    try:
        load_model()
    except Exception as e:
        print(f"âŒ Error loading model: {e}")
        sys.exit(1)

@app.get("/", response_model=HealthResponse)
async def root():
    """Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¶Ø¹ÛŒØª API"""
    return {
        "status": "running",
        "model_loaded": peft_model is not None
    }

@app.get("/health", response_model=HealthResponse)
async def health():
    """Ø¨Ø±Ø±Ø³ÛŒ Ø³Ù„Ø§Ù…Øª API"""
    return {
        "status": "healthy" if peft_model is not None else "model_not_loaded",
        "model_loaded": peft_model is not None
    }

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    Ú†Øª Ø¨Ø§ Ù…Ø¯Ù„
    
    Args:
        request: Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ú†Øª Ø´Ø§Ù…Ù„ message Ùˆ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ generation
    
    Returns:
        Ù¾Ø§Ø³Ø® Ù…Ø¯Ù„
    """
    if peft_model is None or tokenizer is None:
        raise HTTPException(status_code=503, detail="Model not loaded")
    
    if not request.message or not request.message.strip():
        raise HTTPException(status_code=400, detail="Message cannot be empty")
    
    try:
        # Ø³Ø§Ø®Øª prompt
        prompt = f"User: {request.message}\nAssistant:"
        
        # Tokenize
        inputs = tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=512
        ).to(peft_model.device)
        
        # Generate
        with torch.no_grad():
            outputs = peft_model.generate(
                **inputs,
                max_new_tokens=request.max_tokens,
                temperature=request.temperature,
                top_p=request.top_p,
                repetition_penalty=request.repetition_penalty,
                do_sample=True,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
                use_cache=True,
            )
        
        # Decode response
        input_length = inputs["input_ids"].shape[1]
        response = tokenizer.decode(outputs[0][input_length:], skip_special_tokens=True).strip()
        
        # Ø­Ø°Ù prompt Ø§Ú¯Ø± Ø¯Ø± response Ø¨Ø§Ø´Ø¯
        if response.startswith(prompt):
            response = response[len(prompt):].strip()
        
        return ChatResponse(response=response, status="success")
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error generating response: {str(e)}")

@app.post("/chat/simple")
async def chat_simple(message: str):
    """
    Ú†Øª Ø³Ø§Ø¯Ù‡ (ÙÙ‚Ø· message)
    
    Args:
        message: Ù…ØªÙ† Ú©Ø§Ø±Ø¨Ø±
    
    Returns:
        Ù¾Ø§Ø³Ø® Ù…Ø¯Ù„
    """
    request = ChatRequest(message=message)
    return await chat(request)

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Run Chatbot API")
    parser.add_argument("--host", type=str, default="0.0.0.0", help="Host (default: 0.0.0.0)")
    parser.add_argument("--port", type=int, default=8000, help="Port (default: 8000)")
    parser.add_argument("--reload", action="store_true", help="Enable auto-reload")
    
    args = parser.parse_args()
    
    print(f"ğŸš€ Starting API server on http://{args.host}:{args.port}")
    print(f"ğŸ“š API docs: http://{args.host}:{args.port}/docs")
    
    uvicorn.run(
        "api:app",
        host=args.host,
        port=args.port,
        reload=args.reload
    )


#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ø¢Ù…ÙˆØ²Ø´ ÛŒÚ©â€ŒØ¨Ø§Ø±Ù‡ Ù…Ø¯Ù„ - ÙÙ‚Ø· ÛŒÚ© Ø¨Ø§Ø± Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒØ´ÙˆØ¯
One-time training script - runs only once
"""

import os
import json
import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from datasets import Dataset
import sys

# Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ù…Ø¯Ù„ Ø¢Ù…ÙˆØ²Ø´â€ŒØ¯ÛŒØ¯Ù‡
# Check if trained model exists
if os.path.exists("./final_model") and os.path.isdir("./final_model"):
    if os.path.exists("./final_model/config.json"):
        print("âœ… Ù…Ø¯Ù„ Ø§Ø² Ù‚Ø¨Ù„ Ø¢Ù…ÙˆØ²Ø´ Ø¯ÛŒØ¯Ù‡ Ø§Ø³Øª. Ø¢Ù…ÙˆØ²Ø´ Ø±Ø¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
        print("âœ… Model already trained. Skipping training.")
        sys.exit(0)

print("ğŸš€ Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„...")
print("ğŸš€ Starting model training...")

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…Ø¯Ù„ Ù¾Ø§ÛŒÙ‡
# Base model settings
BASE_MODEL = "meta-llama/Llama-3.2-1B-Instruct"  # ÛŒØ§ Llama-3.2-3B-Instruct

# Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Hugging Face
# Check Hugging Face access
hf_token = os.getenv("HF_TOKEN") or os.getenv("HUGGINGFACE_TOKEN")
if not hf_token:
    print("âš ï¸  Warning: HF_TOKEN not set. Trying without token...")
    print("âš ï¸  Ù‡Ø´Ø¯Ø§Ø±: HF_TOKEN ØªÙ†Ø¸ÛŒÙ… Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª. ØªÙ„Ø§Ø´ Ø¨Ø¯ÙˆÙ† ØªÙˆÚ©Ù†...")

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯ÛŒØªØ§Ø³Øª
# Load dataset
print("ğŸ“š Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯ÛŒØªØ§Ø³Øª...")
print("ğŸ“š Loading dataset...")
with open("dataset.json", "r", encoding="utf-8") as f:
    dataset = json.load(f)

print(f"âœ… {len(dataset)} Ù†Ù…ÙˆÙ†Ù‡ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯")
print(f"âœ… {len(dataset)} examples loaded")

# ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ ÙØ±Ù…Øª Ù…Ù†Ø§Ø³Ø¨
# Convert to proper format
def format_prompt(example):
    instruction = example.get("instruction", "")
    input_text = example.get("input", "")
    output = example.get("output", "")
    
    if input_text:
        prompt = f"### Instruction:\n{instruction}\n\n### Input:\n{input_text}\n\n### Response:\n{output}"
    else:
        prompt = f"### Instruction:\n{instruction}\n\n### Response:\n{output}"
    
    return {"text": prompt}

formatted_data = [format_prompt(ex) for ex in dataset]
dataset = Dataset.from_list(formatted_data)

# ØªÙ‚Ø³ÛŒÙ… Ø¨Ù‡ train/validation
# Split to train/validation
dataset = dataset.train_test_split(test_size=0.1, seed=42)
train_dataset = dataset["train"]
eval_dataset = dataset["test"]

print(f"âœ… Ø¯ÛŒØªØ§Ø³Øª ØªÙ‚Ø³ÛŒÙ… Ø´Ø¯: {len(train_dataset)} Ø¢Ù…ÙˆØ²Ø´ØŒ {len(eval_dataset)} Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ")
print(f"âœ… Dataset split: {len(train_dataset)} train, {len(eval_dataset)} eval")

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø±
# Load tokenizer
print("ğŸ”¤ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø±...")
print("ğŸ”¤ Loading tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(
    BASE_MODEL,
    token=hf_token,
    trust_remote_code=True
)

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.pad_token_id = tokenizer.eos_token_id

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª 4-bit quantization
# 4-bit quantization settings
print("âš™ï¸  ØªÙ†Ø¸ÛŒÙ…Ø§Øª quantization...")
print("âš™ï¸  Setting up quantization...")
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
)

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„
# Load model
print("ğŸ¤– Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ù¾Ø§ÛŒÙ‡...")
print("ğŸ¤– Loading base model...")
model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    quantization_config=bnb_config,
    device_map="auto",
    token=hf_token,
    trust_remote_code=True,
    torch_dtype=torch.float16,
)

# Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´
# Prepare model for training
model = prepare_model_for_kbit_training(model)

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª LoRA
# LoRA settings
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)

# Ø§Ø¹Ù…Ø§Ù„ LoRA
# Apply LoRA
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# ØªØ§Ø¨Ø¹ ØªÙˆÚ©Ù†Ø§ÛŒØ² Ú©Ø±Ø¯Ù†
# Tokenization function
def tokenize_function(examples):
    return tokenizer(
        examples["text"],
        truncation=True,
        max_length=512,
        padding="max_length",
    )

# ØªÙˆÚ©Ù†Ø§ÛŒØ² Ú©Ø±Ø¯Ù† Ø¯ÛŒØªØ§Ø³Øª
# Tokenize dataset
print("ğŸ”¤ ØªÙˆÚ©Ù†Ø§ÛŒØ² Ú©Ø±Ø¯Ù† Ø¯ÛŒØªØ§Ø³Øª...")
print("ğŸ”¤ Tokenizing dataset...")
train_dataset = train_dataset.map(tokenize_function, batched=True)
eval_dataset = eval_dataset.map(tokenize_function, batched=True)

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¢Ù…ÙˆØ²Ø´
# Training arguments
training_args = TrainingArguments(
    output_dir="./checkpoints",
    num_train_epochs=3,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    gradient_accumulation_steps=4,
    warmup_steps=50,
    logging_steps=10,
    eval_strategy="steps",
    eval_steps=100,
    save_steps=200,
    save_total_limit=2,
    learning_rate=2e-4,
    fp16=True,
    optim="paged_adamw_8bit",
    report_to="none",
    load_best_model_at_end=True,
    metric_for_best_model="loss",
    greater_is_better=False,
)

# Trainer
print("ğŸ“ Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´...")
print("ğŸ“ Starting training...")
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
)

# Ø¢Ù…ÙˆØ²Ø´
# Train
trainer.train()

# Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ Ù†Ù‡Ø§ÛŒÛŒ
# Save final model
print("ğŸ’¾ Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ Ù†Ù‡Ø§ÛŒÛŒ...")
print("ğŸ’¾ Saving final model...")
model.save_pretrained("./final_model")
tokenizer.save_pretrained("./final_model")

print("âœ… Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯!")
print("âœ… Training completed successfully!")
print("ğŸ“ Ù…Ø¯Ù„ Ø¯Ø± ./final_model Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
print("ğŸ“ Model saved to ./final_model")


#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ø¢Ù…ÙˆØ²Ø´ ÛŒÚ©â€ŒØ¨Ø§Ø±Ù‡ Ù…Ø¯Ù„ - ÙÙ‚Ø· ÛŒÚ© Ø¨Ø§Ø± Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒØ´ÙˆØ¯
One-time training script - runs only once
"""

import os
import json
import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from datasets import Dataset
from huggingface_hub import HfFolder
import sys

# Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ù…Ø¯Ù„ Ø¢Ù…ÙˆØ²Ø´â€ŒØ¯ÛŒØ¯Ù‡
# Check if trained model exists
if os.path.exists("./final_model") and os.path.isdir("./final_model"):
    if os.path.exists("./final_model/config.json"):
        print("âœ… Ù…Ø¯Ù„ Ø§Ø² Ù‚Ø¨Ù„ Ø¢Ù…ÙˆØ²Ø´ Ø¯ÛŒØ¯Ù‡ Ø§Ø³Øª. Ø¢Ù…ÙˆØ²Ø´ Ø±Ø¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
        print("âœ… Model already trained. Skipping training.")
        sys.exit(0)

print("ğŸš€ Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„...")
print("ğŸš€ Starting model training...")

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…Ø¯Ù„ Ù¾Ø§ÛŒÙ‡ - Ù…Ø¯Ù„ Ú©Ø§Ù…Ù„Ø§Ù‹ Ø¨Ø§Ø² Ø¨Ø¯ÙˆÙ† Ù†ÛŒØ§Ø² Ø¨Ù‡ Ù…Ø¬ÙˆØ²
# Base model settings - Fully open model, no license required

# Ú¯Ø²ÛŒÙ†Ù‡ 1: Phi-3-mini (Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ - Ø¨Ù‡ØªØ±ÛŒÙ† Ø¨Ø±Ø§ÛŒ instruction following)
BASE_MODEL = "microsoft/Phi-3-mini-4k-instruct"  # Ú©Ø§Ù…Ù„Ø§Ù‹ Ø¨Ø§Ø²ØŒ Ø¨Ø¯ÙˆÙ† Ù…Ø¬ÙˆØ²ØŒ Ø¹Ø§Ù„ÛŒ Ø¨Ø±Ø§ÛŒ instruction

# Ú¯Ø²ÛŒÙ†Ù‡ 2: Qwen (Ø¹Ø§Ù„ÛŒ Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ/Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ)
# BASE_MODEL = "Qwen/Qwen2-1.5B-Instruct"

# Ú¯Ø²ÛŒÙ†Ù‡ 3: DialoGPT (Ø¨Ø±Ø§ÛŒ dialogue - Ù†ÛŒØ§Ø² Ø¨Ù‡ ÙØ±Ù…Øª Ø®Ø§Øµ)
# BASE_MODEL = "microsoft/DialoGPT-medium"

# Ú¯Ø²ÛŒÙ†Ù‡ 4: GPT2 (Ø¨Ø±Ø§ÛŒ text generation - Ù†ÛŒØ§Ø² Ø¨Ù‡ ÙØ±Ù…Øª Ø®Ø§Øµ)
# BASE_MODEL = "gpt2-medium"

# Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Hugging Face (Ø§Ø®ØªÛŒØ§Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø²)
# Check Hugging Face access (optional for open models)
from huggingface_hub import HfFolder

hf_token = (
    os.getenv("HF_TOKEN") or 
    os.getenv("HUGGINGFACE_TOKEN") or
    HfFolder.get_token()  # Ø®ÙˆØ§Ù†Ø¯Ù† Ø§Ø² Ú©Ø´ Hugging Face
)

# Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø²ØŒ ØªÙˆÚ©Ù† Ø§Ø®ØªÛŒØ§Ø±ÛŒ Ø§Ø³Øª
# For open models, token is optional
if hf_token:
    print("âœ… ØªÙˆÚ©Ù† Hugging Face ÛŒØ§ÙØª Ø´Ø¯ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)")
    print("âœ… Hugging Face token found (optional)")
else:
    print("â„¹ï¸  Ø¨Ø¯ÙˆÙ† ØªÙˆÚ©Ù† Ø§Ø¯Ø§Ù…Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ… (Ù…Ø¯Ù„ Ø¨Ø§Ø² Ø§Ø³Øª)")
    print("â„¹ï¸  Continuing without token (model is open)")

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯ÛŒØªØ§Ø³Øª
# Load dataset
print("ğŸ“š Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯ÛŒØªØ§Ø³Øª...")
print("ğŸ“š Loading dataset...")
with open("dataset.json", "r", encoding="utf-8") as f:
    dataset = json.load(f)

print(f"âœ… {len(dataset)} Ù†Ù…ÙˆÙ†Ù‡ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯")
print(f"âœ… {len(dataset)} examples loaded")

# ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ ÙØ±Ù…Øª Ù…Ù†Ø§Ø³Ø¨ (Ø³Ø§Ø²Ú¯Ø§Ø± Ø¨Ø§ Phi-3 Ùˆ Qwen)
# Convert to proper format (compatible with Phi-3 and Qwen)
def format_prompt(example):
    instruction = example.get("instruction", "")
    input_text = example.get("input", "")
    output = example.get("output", "")
    
    # ÙØ±Ù…Øª Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ instruction following (Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ø¨Ø§ Phi-3, Qwen, Ùˆ Ø¨ÛŒØ´ØªØ± Ù…Ø¯Ù„â€ŒÙ‡Ø§)
    # Standard instruction following format (works with Phi-3, Qwen, and most models)
    if input_text:
        prompt = f"### Instruction:\n{instruction}\n\n### Input:\n{input_text}\n\n### Response:\n{output}"
    else:
        prompt = f"### Instruction:\n{instruction}\n\n### Response:\n{output}"
    
    return {"text": prompt}

formatted_data = [format_prompt(ex) for ex in dataset]
dataset = Dataset.from_list(formatted_data)

# ØªÙ‚Ø³ÛŒÙ… Ø¨Ù‡ train/validation
# Split to train/validation
dataset = dataset.train_test_split(test_size=0.1, seed=42)
train_dataset = dataset["train"]
eval_dataset = dataset["test"]

print(f"âœ… Ø¯ÛŒØªØ§Ø³Øª ØªÙ‚Ø³ÛŒÙ… Ø´Ø¯: {len(train_dataset)} Ø¢Ù…ÙˆØ²Ø´ØŒ {len(eval_dataset)} Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ")
print(f"âœ… Dataset split: {len(train_dataset)} train, {len(eval_dataset)} eval")

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø±
# Load tokenizer
print("ğŸ”¤ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø±...")
print("ğŸ”¤ Loading tokenizer...")
try:
    tokenizer = AutoTokenizer.from_pretrained(
        BASE_MODEL,
        token=hf_token,
        trust_remote_code=True
    )
except Exception as e:
    if "gated" in str(e).lower() or "403" in str(e) or "access" in str(e).lower():
        print("âŒ Ø®Ø·Ø§: Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Ù…Ø¯Ù„ Ù…Ø­Ø¯ÙˆØ¯ Ø§Ø³Øª!")
        print("âŒ Error: Model access is restricted!")
        print("")
        print("ğŸ“ Ù„Ø·ÙØ§ Ø§ÛŒÙ† Ù…Ø±Ø§Ø­Ù„ Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡ÛŒØ¯:")
        print("ğŸ“ Please follow these steps:")
        print("")
        print("1. Ø¨Ù‡ Ø§ÛŒÙ† Ø¢Ø¯Ø±Ø³ Ø¨Ø±ÙˆÛŒØ¯ Ùˆ Ù…Ø¬ÙˆØ² Ø±Ø§ Ø¨Ù¾Ø°ÛŒØ±ÛŒØ¯:")
        print("   Visit and accept the license:")
        print(f"   https://huggingface.co/{BASE_MODEL}")
        print("")
        print("2. Ù…Ø·Ù…Ø¦Ù† Ø´ÙˆÛŒØ¯ Ú©Ù‡ Ø¨Ø§ Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø³Øª Ù„Ø§Ú¯ÛŒÙ† Ú©Ø±Ø¯Ù‡â€ŒØ§ÛŒØ¯:")
        print("   Make sure you're logged in with the correct account:")
        print("   huggingface-cli login")
        print("")
        print("3. Ú†Ù†Ø¯ Ø¯Ù‚ÛŒÙ‚Ù‡ ØµØ¨Ø± Ú©Ù†ÛŒØ¯ ØªØ§ Ø¯Ø³ØªØ±Ø³ÛŒ ÙØ¹Ø§Ù„ Ø´ÙˆØ¯")
        print("   Wait a few minutes for access to be activated")
        print("")
        sys.exit(1)
    else:
        raise

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.pad_token_id = tokenizer.eos_token_id

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª 4-bit quantization
# 4-bit quantization settings
print("âš™ï¸  ØªÙ†Ø¸ÛŒÙ…Ø§Øª quantization...")
print("âš™ï¸  Setting up quantization...")
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
)

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„
# Load model
print("ğŸ¤– Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ù¾Ø§ÛŒÙ‡...")
print("ğŸ¤– Loading base model...")
try:
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        quantization_config=bnb_config,
        device_map="auto",
        token=hf_token,
        trust_remote_code=True,
        torch_dtype=torch.float16,
    )
except Exception as e:
    if "gated" in str(e).lower() or "403" in str(e) or "access" in str(e).lower():
        print("âŒ Ø®Ø·Ø§: Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Ù…Ø¯Ù„ Ù…Ø­Ø¯ÙˆØ¯ Ø§Ø³Øª!")
        print("âŒ Error: Model access is restricted!")
        print("")
        print("ğŸ“ Ù„Ø·ÙØ§ Ø§ÛŒÙ† Ù…Ø±Ø§Ø­Ù„ Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡ÛŒØ¯:")
        print("ğŸ“ Please follow these steps:")
        print("")
        print("1. Ø¨Ù‡ Ø§ÛŒÙ† Ø¢Ø¯Ø±Ø³ Ø¨Ø±ÙˆÛŒØ¯ Ùˆ Ù…Ø¬ÙˆØ² Ø±Ø§ Ø¨Ù¾Ø°ÛŒØ±ÛŒØ¯:")
        print("   Visit and accept the license:")
        print(f"   https://huggingface.co/{BASE_MODEL}")
        print("")
        print("2. Ù…Ø·Ù…Ø¦Ù† Ø´ÙˆÛŒØ¯ Ú©Ù‡ Ø¨Ø§ Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø³Øª Ù„Ø§Ú¯ÛŒÙ† Ú©Ø±Ø¯Ù‡â€ŒØ§ÛŒØ¯:")
        print("   Make sure you're logged in with the correct account:")
        print("   huggingface-cli login")
        print("")
        print("3. Ú†Ù†Ø¯ Ø¯Ù‚ÛŒÙ‚Ù‡ ØµØ¨Ø± Ú©Ù†ÛŒØ¯ ØªØ§ Ø¯Ø³ØªØ±Ø³ÛŒ ÙØ¹Ø§Ù„ Ø´ÙˆØ¯")
        print("   Wait a few minutes for access to be activated")
        print("")
        sys.exit(1)
    else:
        raise

# Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´
# Prepare model for training
model = prepare_model_for_kbit_training(model)

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª LoRA
# LoRA settings
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)

# Ø§Ø¹Ù…Ø§Ù„ LoRA
# Apply LoRA
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# ØªØ§Ø¨Ø¹ ØªÙˆÚ©Ù†Ø§ÛŒØ² Ú©Ø±Ø¯Ù† Ø¨Ø§ labels
# Tokenization function with labels
def tokenize_function(examples):
    # ØªÙˆÚ©Ù†Ø§ÛŒØ² Ú©Ø±Ø¯Ù† Ú©Ù„ Ù…ØªÙ† (prompt + response)
    # Tokenize full text (prompt + response)
    tokenized = tokenizer(
        examples["text"],
        truncation=True,
        max_length=512,
        padding="max_length",
        return_tensors=None,
    )
    
    # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø·ÙˆÙ„ prompt Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù†Ù…ÙˆÙ†Ù‡
    # Find prompt length for each example
    texts = examples["text"]
    if isinstance(texts, str):
        texts = [texts]
    
    labels_list = []
    input_ids = tokenized["input_ids"]
    if not isinstance(input_ids[0], list):
        input_ids = [input_ids]
    
    for i, text in enumerate(texts):
        # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù…ÙˆÙ‚Ø¹ÛŒØª "### Response:" Ø¯Ø± Ù…ØªÙ†
        # Find position of "### Response:" in text
        response_marker = "### Response:\n"
        response_pos = text.find(response_marker)
        
        if response_pos != -1:
            # ØªÙˆÚ©Ù†Ø§ÛŒØ² Ú©Ø±Ø¯Ù† ÙÙ‚Ø· prompt (Ù‚Ø¨Ù„ Ø§Ø² Response)
            # Tokenize only prompt (before Response)
            prompt_text = text[:response_pos + len(response_marker)]
            prompt_tokenized = tokenizer(
                prompt_text,
                truncation=True,
                max_length=512,
                padding=False,
                return_tensors=None,
            )
            prompt_length = len(prompt_tokenized["input_ids"])
        else:
            # Ø§Ú¯Ø± Response Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯ØŒ Ù†ØµÙ Ù…ØªÙ† Ø±Ø§ prompt Ø¯Ø± Ù†Ø¸Ø± Ø¨Ú¯ÛŒØ±
            # If Response not found, consider half text as prompt
            prompt_length = len(input_ids[i]) // 2
        
        # Ø§ÛŒØ¬Ø§Ø¯ labels: ÙÙ‚Ø· Ø¨Ø®Ø´ response Ø¨Ø§ÛŒØ¯ loss Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯
        # Create labels: only response part should have loss
        labels = list(input_ids[i].copy())
        
        # Ù‚Ø³Ù…Øª prompt Ø±Ø§ Ø¯Ø± labels Ø¨Ù‡ -100 ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… (ignore index)
        # Convert prompt part in labels to -100 (ignore index)
        for j in range(min(prompt_length, len(labels))):
            labels[j] = -100
        
        labels_list.append(labels)
    
    tokenized["labels"] = labels_list
    
    # Ø­Ø°Ù ÙÛŒÙ„Ø¯ text (Ø¯ÛŒÚ¯Ø± Ù†ÛŒØ§Ø² Ù†ÛŒØ³Øª)
    # Remove text field (no longer needed)
    if "text" in tokenized:
        del tokenized["text"]
    
    return tokenized

# ØªÙˆÚ©Ù†Ø§ÛŒØ² Ú©Ø±Ø¯Ù† Ø¯ÛŒØªØ§Ø³Øª
# Tokenize dataset
print("ğŸ”¤ ØªÙˆÚ©Ù†Ø§ÛŒØ² Ú©Ø±Ø¯Ù† Ø¯ÛŒØªØ§Ø³Øª...")
print("ğŸ”¤ Tokenizing dataset...")
train_dataset = train_dataset.map(
    tokenize_function, 
    batched=True,
    remove_columns=["text"]  # Ø­Ø°Ù ÙÛŒÙ„Ø¯ text Ø¨Ø¹Ø¯ Ø§Ø² tokenization
)
eval_dataset = eval_dataset.map(
    tokenize_function, 
    batched=True,
    remove_columns=["text"]  # Ø­Ø°Ù ÙÛŒÙ„Ø¯ text Ø¨Ø¹Ø¯ Ø§Ø² tokenization
)

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¢Ù…ÙˆØ²Ø´
# Training arguments
training_args = TrainingArguments(
    output_dir="./checkpoints",
    num_train_epochs=3,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    gradient_accumulation_steps=4,
    warmup_steps=50,
    logging_steps=10,
    eval_strategy="steps",
    eval_steps=100,
    save_steps=200,
    save_total_limit=2,
    learning_rate=2e-4,
    fp16=True,
    optim="paged_adamw_8bit",
    report_to="none",
    load_best_model_at_end=True,
    metric_for_best_model="loss",
    greater_is_better=False,
    remove_unused_columns=False,  # Ø¨Ø±Ø§ÛŒ Ø­ÙØ¸ labels
    dataloader_pin_memory=False,  # Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ù…Ø´Ú©Ù„Ø§Øª Ø­Ø§ÙØ¸Ù‡
)

# Trainer
print("ğŸ“ Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´...")
print("ğŸ“ Starting training...")
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
)

# Ø¢Ù…ÙˆØ²Ø´
# Train
trainer.train()

# Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ Ù†Ù‡Ø§ÛŒÛŒ
# Save final model
print("ğŸ’¾ Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ Ù†Ù‡Ø§ÛŒÛŒ...")
print("ğŸ’¾ Saving final model...")
model.save_pretrained("./final_model")
tokenizer.save_pretrained("./final_model")

print("âœ… Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯!")
print("âœ… Training completed successfully!")
print("ğŸ“ Ù…Ø¯Ù„ Ø¯Ø± ./final_model Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
print("ğŸ“ Model saved to ./final_model")


# ุฑุงูููุง ฺฉุงูู ูุงูโุชูู ุจุฑุง RTX 3080
# Complete Fine-tuning Guide for RTX 3080

## ๐ฏ ูุฏู

ูุงูโุชูู ฺฉ ูุฏู 8B ูุงุฑุณ/ูุนูู ุฑู RTX 3080 (10GB) ุจุง QLoRA ุจุฑุง:
- โ ฺุชโุจุงุช ุทุจุนโุชุฑ ู ุงุญุณุงุณโุชุฑ
- โ ุดุจูโุณุงุฒ ุฏูู ูุงุฏุฑ/ุนุฒุฒุงู ููุชโุดุฏู
- โ ุงุณุชูุงุฏู ุจููู ุงุฒ ุญุงูุธู GPU

## ๐ ูพุดโูุงุฒูุง

### ุณุฎุชโุงูุฒุงุฑ:
- RTX 3080 10GB (ุง GPU ูุดุงุจู)
- ุญุฏุงูู 16GB RAM
- ูุถุง ุฏุณฺฉ: 20GB+

### ูุฑูโุงูุฒุงุฑ:
- Python 3.10+
- CUDA 11.8 ุง 12.1
- PyTorch ุจุง CUDA

## ๐ ูุตุจ

### 1. ูุตุจ PyTorch ุจุง CUDA

```bash
# ุจุฑุง CUDA 11.8
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# ุจุฑุง CUDA 12.1
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

### 2. ูุตุจ ูุงุจุณุชฺฏโูุง

```bash
pip install -r requirements_3080.txt
```

### 3. ูุตุจ Flash Attention (ุงุฎุชุงุฑ - ุจุฑุง ุณุฑุนุช ุจุดุชุฑ)

```bash
# ูุงุฒ ุจู CUDA toolkit ุฏุงุฑุฏ
pip install flash-attn --no-build-isolation
```

## ๐ ุขูุงุฏูโุณุงุฒ ุฏุชุงุณุช

### 1. ุชุจุฏู ุฏุชุงุณุช ุจู ูุฑูุช ChatML

```bash
python scripts/dataset_prep.py
```

ุงู ุงุณฺฉุฑูพุช:
- ุฏุชุงุณุช ุฑุง ุงุฒ `data/dataset.json` ูโุฎูุงูุฏ
- ุจู ูุฑูุช ChatML ุชุจุฏู ูโฺฉูุฏ
- ุฏุฑ `data/data_persian.json` ุฐุฎุฑู ูโฺฉูุฏ

### 2. ูุฑูุช ุฏุชุงุณุช

ุฏุชุงุณุช ุจุงุฏ ุจู ุงู ูุฑูุช ุจุงุดุฏ:

```json
[
  {
    "messages": [
      {
        "role": "user",
        "content": "ูุงูุงู ุฌููุ ุงูุฑูุฒ ุฎู ุฏูู ฺฏุฑูุชู..."
      },
      {
        "role": "assistant",
        "content": "ุนุฒุฒ ุฏููุ ุจุง ูพุด ูุงูุงู. ุชู ฺฉู ูโุฏูู ูฺโููุช ุชููุงุช ููโุฐุงุฑู. ุจฺฏู ฺ ุดุฏู ุฌูุฌูโุงูุ"
      }
    ]
  }
]
```

### 3. ุจุฑุฑุณ ุฏุชุงุณุช

```bash
# ุจุฑุฑุณ ุชุนุฏุงุฏ ูููููโูุง
python -c "import json; data = json.load(open('data/data_persian.json', 'r', encoding='utf-8')); print(f'Total: {len(data)} examples')"
```

**ุชูุตู**: ุญุฏุงูู 500-2000 ููููู ุจุฑุง ูุชุฌู ุฎูุจ

## ๐ ูุงูโุชูู

### ุงุฌุฑุง ูุงูโุชูู

```bash
python scripts/train_3080.py
```

### ุชูุธูุงุช ุจููู ุจุฑุง RTX 3080

ุฏุฑ `scripts/train_3080.py`:

```python
# LoRA
LORA_R = 64              # ุฑุชุจู LoRA (ุจุงูุงุชุฑ = ฺฉูุช ุจูุชุฑ)
LORA_ALPHA = 16          # Alpha ุจุฑุง LoRA
LORA_DROPOUT = 0.1       # Dropout

# Batch
BATCH_SIZE = 3           # per device
GRADIENT_ACCUMULATION = 4 # effective batch = 12

# Training
LEARNING_RATE = 2e-4     # Learning rate
NUM_EPOCHS = 5           # ุชุนุฏุงุฏ epoch
MAX_SEQ_LENGTH = 2048    # ุญุฏุงฺฉุซุฑ ุทูู sequence
```

### ุฒูุงู ุขููุฒุด

- ุจุง 1000 ููููู: ุญุฏูุฏ 2-3 ุณุงุนุช
- ุจุง 2000 ููููู: ุญุฏูุฏ 4-5 ุณุงุนุช

### ูุธุงุฑุช ุจุฑ ุขููุฒุด

ูุงฺฏโูุง ุฏุฑ `checkpoints_3080/` ุฐุฎุฑู ูโุดููุฏ. ูโุชูุงูุฏ loss ุฑุง ุจุฑุฑุณ ฺฉูุฏ:

```bash
# ูุดุงูุฏู ูุงฺฏโูุง
tail -f checkpoints_3080/training.log
```

## ๐งช ุชุณุช ู Inference

### 1. ุจุง Gradio (ูพุดููุงุฏ)

```bash
python scripts/inference_3080.py
```

ุณูพุณ ูุฑูุฑฺฏุฑ ุฑุง ุจุงุฒ ฺฉูุฏ: `http://localhost:7860`

### 2. ุจุง Python

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel
import torch

BASE_MODEL = "meta-llama/Meta-Llama-3.1-8B-Instruct"
MODEL_PATH = "models/llama3_8b_finetuned"

# ุจุงุฑฺฏุฐุงุฑ
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
)

base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    quantization_config=bnb_config,
    device_map="auto",
    torch_dtype=torch.bfloat16,
)

model = PeftModel.from_pretrained(base_model, MODEL_PATH)
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)

# ฺุช
prompt = "<|user|>\nุณูุงู ูุงูุงู<|end|>\n<|assistant|>\n"
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

with torch.no_grad():
    outputs = model.generate(**inputs, max_new_tokens=300)

response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)
```

## โ๏ธ ุชูุธูุงุช ูพุดุฑูุชู

### ุงูุฒุงุด ฺฉูุช (ุงฺฏุฑ ุญุงูุธู ฺฉุงู ุฏุงุฑุฏ)

```python
LORA_R = 128  # ุงุฒ 64 ุจู 128
LORA_ALPHA = 32  # ุงุฒ 16 ุจู 32
BATCH_SIZE = 2  # ฺฉุงูุด batch ุจุฑุง ุฌุง ุฏุงุฏู
```

### ุงูุฒุงุด ุณุฑุนุช

```python
BATCH_SIZE = 4  # ุงูุฒุงุด batch
GRADIENT_ACCUMULATION = 2  # ฺฉุงูุด accumulation
MAX_SEQ_LENGTH = 1024  # ฺฉุงูุด ุทูู sequence
```

### ฺฉุงูุด ุญุงูุธู

```python
BATCH_SIZE = 1  # ฺฉุงูุด batch
GRADIENT_ACCUMULATION = 8  # ุงูุฒุงุด accumulation
MAX_SEQ_LENGTH = 512  # ฺฉุงูุด ุทูู sequence
```

## ๐ ุนุจโุงุจ

### ุฎุทุง: "CUDA out of memory"

**ุฑุงูโุญู:**
1. `BATCH_SIZE` ุฑุง ฺฉุงูุด ุฏูุฏ (3 โ 2 โ 1)
2. `MAX_SEQ_LENGTH` ุฑุง ฺฉุงูุด ุฏูุฏ (2048 โ 1024)
3. `gradient_checkpointing=True` ุฑุง ูุนุงู ฺฉูุฏ (ูุนุงู ุงุณุช)

### ุฎุทุง: "Model not found"

**ุฑุงูโุญู:**
```bash
# ุฏุงูููุฏ ูุฏู ุงุฒ Hugging Face
huggingface-cli login
# ุณูพุณ ุฏูุจุงุฑู train ฺฉูุฏ
```

### ุฎุทุง: "Flash Attention not available"

**ุฑุงูโุญู:**
- ุงู ุฎุทุง ููู ูุณุชุ ูุฏู ุจุง `eager` attention ูู ฺฉุงุฑ ูโฺฉูุฏ
- ุงฺฏุฑ ูโุฎูุงูุฏ Flash Attention ุฏุงุดุชู ุจุงุดุฏุ CUDA toolkit ุฑุง ูุตุจ ฺฉูุฏ

## ๐ ุจุฑุฑุณ ูุชุงุฌ

### ูุนุงุฑูุง ฺฉูุช:

1. **Loss**: ุจุงุฏ ฺฉุงูุด ุงุจุฏ (ูุซูุงู ุงุฒ 2.5 ุจู 1.5)
2. **Perplexity**: ุจุงุฏ ฺฉุงูุด ุงุจุฏ
3. **ูพุงุณุฎโูุง**: ุจุงุฏ ุทุจุนโุชุฑ ู ุงุญุณุงุณโุชุฑ ุจุงุดูุฏ

### ุชุณุช ุฏุณุช:

ฺูุฏ ุณูุงู ุจูพุฑุณุฏ ู ุจุฑุฑุณ ฺฉูุฏ:
- โ ูพุงุณุฎโูุง ุทุจุน ูุณุชูุฏุ
- โ ุงุญุณุงุณ ู ุดุฎุต ูุณุชูุฏุ
- โ ุดุจู ูุงุฏุฑ/ุนุฒุฒ ููุชโุดุฏู ูุณุชูุฏุ

## ๐ก ูฺฉุงุช ููู

1. **ฺฉูุช ุฏุชุงุณุช**: ูููโุชุฑ ุงุฒ ุชุนุฏุงุฏ ุงุณุช
2. **ุตุจุฑ**: ูุงูโุชูู ุฒูุงู ูโุจุฑุฏ (2-5 ุณุงุนุช)
3. **ูุธุงุฑุช**: loss ุฑุง ุจุฑุฑุณ ฺฉูุฏ
4. **ุชุณุช**: ุจุนุฏ ุงุฒ ุขููุฒุด ุญุชูุงู ุชุณุช ฺฉูุฏ

## ๐ ุฎูุงุตู ุฏุณุชูุฑุงุช

```bash
# 1. ุขูุงุฏูโุณุงุฒ ุฏุชุงุณุช
python scripts/dataset_prep.py

# 2. ูุงูโุชูู
python scripts/train_3080.py

# 3. ุชุณุช
python scripts/inference_3080.py
```

## โ ฺฺฉโูุณุช

- [ ] PyTorch ุจุง CUDA ูุตุจ ุดุฏู
- [ ] ูุงุจุณุชฺฏโูุง ูุตุจ ุดุฏูโุงูุฏ
- [ ] ุฏุชุงุณุช ุขูุงุฏู ุงุณุช (data_persian.json)
- [ ] ุญุฏุงูู 500 ููููู ุฏุฑ ุฏุชุงุณุช
- [ ] GPU ุฏุฑ ุฏุณุชุฑุณ ุงุณุช
- [ ] ูุถุง ฺฉุงู ุจุฑุง ุฐุฎุฑู ูุฏู

## ๐ฏ ูุชุฌู

ุจุนุฏ ุงุฒ ูุงูโุชูู:
- โ ูุฏู ุทุจุนโุชุฑ ุตุญุจุช ูโฺฉูุฏ
- โ ุงุญุณุงุณโุชุฑ ู ุดุฎุตโุชุฑ ุงุณุช
- โ ุดุจูโุณุงุฒ ุฏููโุชุฑ ูุงุฏุฑ/ุนุฒุฒ ููุชโุดุฏู
- โ ุงุณุชูุงุฏู ุจููู ุงุฒ RTX 3080


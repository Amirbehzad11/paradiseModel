# ๐ ุฑุงูููุง ุณุฑุน Fine-tuning ุญุฑููโุง ุจุฑุง RTX 3080 10GB

## ๐ ูพุดโูุงุฒูุง

1. **ูุตุจ PyTorch ุจุง CUDA:**
   ```bash
   # ุจุฑุง CUDA 11.8
   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
   
   # ุง ุจุฑุง CUDA 12.1
   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
   ```

2. **ูุตุจ ูุงุจุณุชฺฏโูุง:**
   ```bash
   pip install -r requirements_3080.txt
   ```

## ๐ฏ ุขููุฒุด ูุฏู

```bash
python scripts/train_3080.py
```

**ุฒูุงู ุชูุฑุจ:** 1.5 ุชุง 2.5 ุณุงุนุช

**ุฎุฑูุฌ:** `/home/arisa/paradiseModel/models/final_model/llama3_8b_persian_paradise`

## ๐ฌ ุงุณุชูุงุฏู ุงุฒ ูุฏู (Inference)

```bash
python scripts/inference_gradio.py
```

ุณูพุณ ูุฑูุฑฺฏุฑ ุฑุง ุจุงุฒ ฺฉูุฏ ู ุจู `http://localhost:7860` ุจุฑูุฏ.

## โ๏ธ ุชูุธูุงุช ุจููู

- **4-bit Quantization** ุจุง bitsandbytes
- **QLoRA** ุจุง r=64, alpha=16
- **Gradient Checkpointing** ุจุฑุง ุตุฑููโุฌู ุฏุฑ ุญุงูุธู
- **FP16** ุจุฑุง ุณุฑุนุช ุจุดุชุฑ
- **Paged AdamW 8-bit** optimizer

## ๐ ูฺฉุงุช ููู

1. ูุฏู ุจู ุตูุฑุช ุฎูุฏฺฉุงุฑ ุจู `Meta-Llama-3-8B-Instruct` ู `Hermes-2-Pro-Mistral-7B` ุงูุชุฎุงุจ ูโฺฉูุฏ
2. System Prompt ุงุญุณุงุณ ุจู ุตูุฑุช ุฎูุฏฺฉุงุฑ ุจู ููู ูููููโูุง ุงุถุงูู ูโุดูุฏ
3. ูุฏู ุฏุฑ ุตูุฑุช ฺฉูุจูุฏ ุญุงูุธู GPUุ ุงุฒ CPU offloading ุงุณุชูุงุฏู ูโฺฉูุฏ

## ๐จ ูฺฺฏโูุง Inference

- โ ูพุดุชุจุงู ุงุฒ ุขูพููุฏ ุนฺฉุณ
- โ ฺุช ุจุง ุชุงุฑุฎฺู
- โ ุชูุธูุงุช ูพุดุฑูุชู Generation
- โ ุฑุงุจุท ฺฉุงุฑุจุฑ ุฒุจุง ุจุง ูููุช Vazir

